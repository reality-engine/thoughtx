{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi\n",
      "  Obtaining dependency information for fastapi from https://files.pythonhosted.org/packages/4d/d2/3ad038a2365fefbac19d9a046cab7ce45f4c7bfa81d877cbece9707de9ce/fastapi-0.103.2-py3-none-any.whl.metadata\n",
      "  Downloading fastapi-0.103.2-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: torch in /opt/homebrew/lib/python3.11/site-packages (2.0.1)\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/1a/d1/3bba59606141ae808017f6fde91453882f931957f125009417b87a281067/transformers-4.34.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.34.0-py3-none-any.whl.metadata (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting uvicorn\n",
      "  Obtaining dependency information for uvicorn from https://files.pythonhosted.org/packages/79/96/b0882a1c3f7ef3dd86879e041212ae5b62b4bd352320889231cc735a8e8f/uvicorn-0.23.2-py3-none-any.whl.metadata\n",
      "  Using cached uvicorn-0.23.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting google-cloud-storage\n",
      "  Obtaining dependency information for google-cloud-storage from https://files.pythonhosted.org/packages/3a/9f/7923b9e460023470826d124156503359bf77ee130adb0872570599e8cd98/google_cloud_storage-2.11.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_storage-2.11.0-py2.py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/db/0d/1f6d2cd52c886707b00ddb7ed2504cbf10903a60a7bebcd71f0f77d53505/scikit_learn-1.3.1-cp311-cp311-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading scikit_learn-1.3.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting anyio<4.0.0,>=3.7.1 (from fastapi)\n",
      "  Obtaining dependency information for anyio<4.0.0,>=3.7.1 from https://files.pythonhosted.org/packages/19/24/44299477fe7dcc9cb58d0a57d5a7588d6af2ff403fdd2d47a246c91a3246/anyio-3.7.1-py3-none-any.whl.metadata\n",
      "  Using cached anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /opt/homebrew/lib/python3.11/site-packages (from fastapi) (1.10.11)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi)\n",
      "  Obtaining dependency information for starlette<0.28.0,>=0.27.0 from https://files.pythonhosted.org/packages/58/f8/e2cca22387965584a409795913b774235752be4176d276714e15e1a58884/starlette-0.27.0-py3-none-any.whl.metadata\n",
      "  Using cached starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/homebrew/lib/python3.11/site-packages (from fastapi) (4.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michaelholborn/Library/Python/3.11/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (1.25.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/michaelholborn/Library/Python/3.11/lib/python/site-packages (from transformers) (23.1)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Obtaining dependency information for pyyaml>=5.1 from https://files.pythonhosted.org/packages/28/09/55f715ddbf95a054b764b547f617e22f1d5e45d83905660e9a088078fe67/PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.15,>=0.14 from https://files.pythonhosted.org/packages/7e/8e/9c0f7799da9a690ec29a7a7b6c0744d3f735e40951d2f62c8202faf3df6a/tokenizers-0.14.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading tokenizers-0.14.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/91/4f/bded5d0435a72f1bd39f5aaa81077d84fa39526a7c055cc8b8aa44fce681/safetensors-0.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading safetensors-0.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/homebrew/lib/python3.11/site-packages (from uvicorn) (8.1.4)\n",
      "Collecting h11>=0.8 (from uvicorn)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Collecting google-auth<3.0dev,>=1.25.0 (from google-cloud-storage)\n",
      "  Obtaining dependency information for google-auth<3.0dev,>=1.25.0 from https://files.pythonhosted.org/packages/39/7c/2e4fa55a99f83ef9ef229ac5d59c44ceb90e2d0145711590c0fa39669f32/google_auth-2.23.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.23.3-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 (from google-cloud-storage)\n",
      "  Obtaining dependency information for google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 from https://files.pythonhosted.org/packages/4d/ce/4fd62ea66b3508debc795e475336ce915929765870f0ad52328426ba016e/google_api_core-2.12.0-py3-none-any.whl.metadata\n",
      "  Downloading google_api_core-2.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0 (from google-cloud-storage)\n",
      "  Obtaining dependency information for google-cloud-core<3.0dev,>=2.3.0 from https://files.pythonhosted.org/packages/a2/40/02045f776fdb6e44194f34b6375a26ce8a61bd9bd03cd8930ed91cf51a62/google_cloud_core-2.3.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_core-2.3.3-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting google-resumable-media>=2.6.0 (from google-cloud-storage)\n",
      "  Obtaining dependency information for google-resumable-media>=2.6.0 from https://files.pythonhosted.org/packages/c7/4f/b8e5e22406e5aeafa46df8799939d5eeee52f18eeed339675167fac6603a/google_resumable_media-2.6.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_resumable_media-2.6.0-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.3.1)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/lib/python3.11/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (3.4)\n",
      "Collecting sniffio>=1.1 (from anyio<4.0.0,>=3.7.1->fastapi)\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage)\n",
      "  Obtaining dependency information for googleapis-common-protos<2.0.dev0,>=1.56.2 from https://files.pythonhosted.org/packages/a7/bc/416a1ffeba4dcd072bc10523dac9ed97f2e7fc4b760580e2bdbdc1e2afdd/googleapis_common_protos-1.60.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage)\n",
      "  Obtaining dependency information for protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 from https://files.pythonhosted.org/packages/88/12/efb5896c901382548ecb58d0449885a8f9aa62bb559d65e5a8a47f122629/protobuf-4.24.4-cp37-abi3-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading protobuf-4.24.4-cp37-abi3-macosx_10_9_universal2.whl.metadata (540 bytes)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0dev,>=1.25.0->google-cloud-storage)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0dev,>=1.25.0->google-cloud-storage)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0dev,>=1.25.0->google-cloud-storage)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-resumable-media>=2.6.0->google-cloud-storage)\n",
      "  Using cached google_crc32c-1.5.0-cp311-cp311-macosx_10_9_universal2.whl (32 kB)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.16.4->transformers)\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/fe/d3/e1aa96437d944fbb9cc95d0316e25583886e9cd9e6adc07baad943524eda/fsspec-2023.9.2-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.9.2-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/michaelholborn/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage)\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Using cached fastapi-0.103.2-py3-none-any.whl (66 kB)\n",
      "Using cached transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
      "Using cached uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
      "Using cached google_cloud_storage-2.11.0-py2.py3-none-any.whl (118 kB)\n",
      "Using cached scikit_learn-1.3.1-cp311-cp311-macosx_12_0_arm64.whl (9.4 MB)\n",
      "Using cached anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "Using cached google_api_core-2.12.0-py3-none-any.whl (121 kB)\n",
      "Downloading google_auth-2.23.3-py2.py3-none-any.whl (182 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB)\n",
      "Using cached google_resumable_media-2.6.0-py2.py3-none-any.whl (80 kB)\n",
      "Using cached huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "Using cached PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl (167 kB)\n",
      "Using cached safetensors-0.4.0-cp311-cp311-macosx_11_0_arm64.whl (425 kB)\n",
      "Using cached starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Using cached tokenizers-0.14.1-cp311-cp311-macosx_11_0_arm64.whl (2.5 MB)\n",
      "Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB)\n",
      "Using cached protobuf-4.24.4-cp37-abi3-macosx_10_9_universal2.whl (409 kB)\n",
      "Using cached fsspec-2023.9.2-py3-none-any.whl (173 kB)\n",
      "Installing collected packages: threadpoolctl, sniffio, safetensors, pyyaml, pyasn1, protobuf, h11, google-crc32c, fsspec, cachetools, uvicorn, scikit-learn, rsa, pyasn1-modules, huggingface-hub, googleapis-common-protos, google-resumable-media, anyio, tokenizers, starlette, google-auth, transformers, google-api-core, fastapi, google-cloud-core, google-cloud-storage\n",
      "Successfully installed anyio-3.7.1 cachetools-5.3.1 fastapi-0.103.2 fsspec-2023.9.2 google-api-core-2.12.0 google-auth-2.23.3 google-cloud-core-2.3.3 google-cloud-storage-2.11.0 google-crc32c-1.5.0 google-resumable-media-2.6.0 googleapis-common-protos-1.60.0 h11-0.14.0 huggingface-hub-0.17.3 protobuf-4.24.4 pyasn1-0.5.0 pyasn1-modules-0.3.0 pyyaml-6.0.1 rsa-4.9 safetensors-0.4.0 scikit-learn-1.3.1 sniffio-1.3.0 starlette-0.27.0 threadpoolctl-3.2.0 tokenizers-0.14.1 transformers-4.34.0 uvicorn-0.23.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install fastapi pandas torch transformers uvicorn google-cloud-storage scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, UploadFile, HTTPException\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import io\n",
    "from transformers import BartTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
    "\n",
    "class BrainTranslator(nn.Module):\n",
    "    def __init__(self, pretrained_layers, in_feature=840, decoder_embedding_size=1024, additional_encoder_nhead=8, additional_encoder_dim_feedforward=2048):\n",
    "        super(BrainTranslator, self).__init__()\n",
    "        \n",
    "        self.pretrained = pretrained_layers\n",
    "        self.additional_encoder_layer = nn.TransformerEncoderLayer(d_model=in_feature, nhead=additional_encoder_nhead, dim_feedforward=additional_encoder_dim_feedforward, batch_first=True)\n",
    "        self.additional_encoder = nn.TransformerEncoder(self.additional_encoder_layer, num_layers=6)\n",
    "        self.fc1 = nn.Linear(in_feature, decoder_embedding_size)\n",
    "\n",
    "    def forward(self, input_embeddings_batch, input_masks_batch, input_masks_invert):\n",
    "        encoded_embedding = self.additional_encoder(input_embeddings_batch, src_key_padding_mask=input_masks_invert)\n",
    "        encoded_embedding = F.relu(self.fc1(encoded_embedding))\n",
    "        out = self.pretrained(inputs_embeds=encoded_embedding, attention_mask=input_masks_batch, return_dict=True)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "_MODEL = None\n",
    "\n",
    "def get_model():\n",
    "    global _MODEL\n",
    "\n",
    "    if _MODEL is None:\n",
    "        pretrained_bart = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "        checkpoint_path = '/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/task1_task2_taskNRv2_finetune_BrainTranslator_skipstep1_b1_20_30_5e-05_5e-07_unique_sent.pt'  # Change to the path of your model\n",
    "        _MODEL = BrainTranslator(pretrained_bart)\n",
    "        model_weights = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "        _MODEL.load_state_dict(model_weights)\n",
    "        _MODEL.eval()\n",
    "\n",
    "    return _MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess_eeg_data_for_inference(raw_eeg_data: np.ndarray, segment: bool = False, segment_length: int = 128) -> np.ndarray:\n",
    "    eeg_data_df = pd.DataFrame(raw_eeg_data)\n",
    "    eeg_data_filled = eeg_data_df.fillna(eeg_data_df.mean())\n",
    "    scaler = StandardScaler()\n",
    "    normalized_data = scaler.fit_transform(eeg_data_filled)\n",
    "    if segment:\n",
    "        segmented_data = segment_eeg_data(normalized_data, segment_length)\n",
    "        return segmented_data\n",
    "    print(\"Segment Data: \",segment)\n",
    "    print(\"Normalised Data: \",normalized_data)\n",
    "\n",
    "    return normalized_data\n",
    "\n",
    "def segment_eeg_data(eeg_data: np.ndarray, segment_length: int = 128) -> np.ndarray:\n",
    "    num_segments = eeg_data.shape[1] // segment_length\n",
    "    segmented_data = []\n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * segment_length\n",
    "        end_idx = (i + 1) * segment_length\n",
    "        segment = eeg_data[:, start_idx:end_idx]\n",
    "        segmented_data.append(segment)\n",
    "    return np.array(segmented_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open(\"datasets/results.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "raw_eeg_data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment Data:  False\n",
      "Normalised Data:  [[-0.12980619 -0.25275585 -0.15944155 ...  0.92220916  0.12201108\n",
      "  -0.43434176]\n",
      " [ 0.02935555 -0.49875623 -0.50513756 ...  1.62593771  0.4048283\n",
      "  -0.4977017 ]\n",
      " [-0.68281424 -1.04184985 -0.89878684 ...  1.64638032 -0.21750464\n",
      "  -0.85330254]\n",
      " ...\n",
      " [-0.5933969  -0.83465343 -0.66745245 ...  1.21945275 -0.01502839\n",
      "  -0.20512466]\n",
      " [ 0.20163897 -0.38502061 -0.3968381  ...  1.36628486 -0.14704242\n",
      "  -0.5807963 ]\n",
      " [-0.55645603 -0.13538309  0.14307366 ...  0.37879432  0.09646755\n",
      "  -0.3338418 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "eeg_tensor = preprocess_eeg_data_for_inference(raw_eeg_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =get_model()\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text_from_eeg(input_sample: dict, model, tokenizer, device=\"cpu\") -> str:\n",
    "    \"\"\"\n",
    "    Generate text from preprocessed EEG data using a trained model.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_sample: The prepared input sample.\n",
    "    - model: The trained EEG-to-text model.\n",
    "    - tokenizer: The BART tokenizer.\n",
    "    - device: The device to run the model on (e.g., \"cpu\", \"cuda\").\n",
    "    \n",
    "    Returns:\n",
    "    - Generated text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Move the sample and model to the specified device\n",
    "    input_sample[\"sent_level_EEG\"] = input_sample[\"sent_level_EEG\"].to(device)\n",
    "    input_sample[\"target_ids\"] = input_sample[\"target_ids\"].to(device)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"input_sample: \",input_sample)\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=None, encoder_outputs=(input_sample[\"sent_level_EEG\"], None), decoder_input_ids=input_sample[\"target_ids\"])\n",
    "    \n",
    "    # Extract the generated token IDs from the model's outputs\n",
    "    generated_ids = outputs.logits.argmax(dim=-1)\n",
    "    \n",
    "    # Decode the token IDs to text\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Ensure input_sample is structured correctly\n",
    "Before calling generate_text_from_eeg, the input_sample should be structured as a dictionary with keys sent_level_EEG and target_ids. We need to check if this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BrainTranslator.forward() got an unexpected keyword argument 'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/notebook.ipynb Cell 11\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/notebook.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m placeholder_token \u001b[39m=\u001b[39m tokenizer(\u001b[39m\"\u001b[39m\u001b[39m<s>\u001b[39m\u001b[39m\"\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/notebook.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m input_sample \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/notebook.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msent_level_EEG\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mtensor(eeg_tensor),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/notebook.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtarget_ids\u001b[39m\u001b[39m\"\u001b[39m: placeholder_token[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/notebook.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m }\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/notebook.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m results_json \u001b[39m=\u001b[39m generate_text_from_eeg(input_sample,model,tokenizer)\n",
      "\u001b[1;32m/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/notebook.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/notebook.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Perform inference\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/notebook.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/notebook.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, encoder_outputs\u001b[39m=\u001b[39;49m(input_sample[\u001b[39m\"\u001b[39;49m\u001b[39msent_level_EEG\u001b[39;49m\u001b[39m\"\u001b[39;49m], \u001b[39mNone\u001b[39;49;00m), decoder_input_ids\u001b[39m=\u001b[39;49minput_sample[\u001b[39m\"\u001b[39;49m\u001b[39mtarget_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/notebook.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Extract the generated token IDs from the model's outputs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/notebook.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m generated_ids \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: BrainTranslator.forward() got an unexpected keyword argument 'input_ids'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocess the data\n",
    "placeholder_token = tokenizer(\"<s>\", return_tensors=\"pt\")\n",
    "input_sample = {\n",
    "    \"sent_level_EEG\": torch.tensor(eeg_tensor),\n",
    "    \"target_ids\": placeholder_token[\"input_ids\"]\n",
    "}\n",
    "\n",
    "results_json = generate_text_from_eeg(input_sample,model,tokenizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
