{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # add parent directory to system path\n",
    "import torch\n",
    "import json\n",
    "from transformers import BartTokenizer\n",
    "from model.model_loader import get_model\n",
    "\n",
    "# Re-importing the necessary libraries\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.zuco_data import ZuCo_dataset, get_input_sample,eval_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_json_file(raw, filename):\n",
    "    with open(raw + filename, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_datasets(task_names,root):\n",
    "    \"\"\" Load datasets for the given tasks \"\"\"\n",
    "    task_paths = {\n",
    "        'task1': os.path.join(root, 'task1-SR/pickle/task1-SR-dataset.pickle'),\n",
    "        'task2': os.path.join(root, 'task2-NR/pickle/task2-NR-dataset.pickle'),\n",
    "        'task3': os.path.join(root, 'task3-TSR/pickle/task3-TSR-dataset.pickle'),\n",
    "        'taskNRv2': os.path.join(root, 'task2-NR-2.0/pickle/task2-NR-2.0-dataset.pickle')\n",
    "    }\n",
    "    \n",
    "    whole_dataset_dicts = []\n",
    "    for task_name in task_names:\n",
    "        if task_name in task_paths:\n",
    "            with open(task_paths[task_name], 'rb') as handle:\n",
    "                whole_dataset_dicts.append(pickle.load(handle))\n",
    "\n",
    "    return whole_dataset_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_simple(dataloaders, device, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a given dataset and returns the predicted strings for inspection.\n",
    "    \n",
    "    Args:\n",
    "    - dataloaders (dict): A dictionary containing the dataloaders for different sets ('test' in this case).\n",
    "    - device (torch.device): The device to run the model on.\n",
    "    - tokenizer (transformers tokenizer): The tokenizer used for decoding the model outputs.\n",
    "    - model (nn.Module): The model to be evaluated.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list containing the predicted strings for inspection.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()  # Set model to evaluate mode\n",
    "    predicted_strings = []  # Store the predicted strings\n",
    "\n",
    "    # Process only 10 batches from the dataloader\n",
    "    for i, (input_embeddings, seq_len, input_masks, input_mask_invert, target_ids, _, _, _) in enumerate(dataloaders['test']):\n",
    "        if i >= 10:\n",
    "            break\n",
    "\n",
    "        # Transfer data to the specified device\n",
    "        input_embeddings_batch = input_embeddings.to(device).float()\n",
    "        input_masks_batch = input_masks.to(device)\n",
    "        target_ids_batch = target_ids.to(device)\n",
    "        input_mask_invert_batch = input_mask_invert.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        seq2seqLMoutput = model(input_embeddings_batch, input_masks_batch, input_mask_invert_batch, target_ids_batch)\n",
    "        logits = seq2seqLMoutput.logits\n",
    "        probs = logits[0].softmax(dim=1)\n",
    "        _, predictions = probs.topk(1)\n",
    "        predictions = torch.squeeze(predictions)\n",
    "        predicted_string = tokenizer.decode(predictions).split('</s></s>')[0].replace('<s>', '')\n",
    "        \n",
    "        # Append the predicted string to the list\n",
    "        predicted_strings.append(predicted_string)\n",
    "\n",
    "    return predicted_strings\n",
    "\n",
    "# This is the refactored function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]subjects: ALL\n",
      "[INFO]eeg type: GD\n",
      "[INFO]using bands: ['_t1', '_t2', '_a1', '_a2', '_b1', '_b2', '_g1', '_g2']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "checkpoint_path = \"/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/local_checkpoint/task1_task2_taskNRv2_finetune_BrainTranslator_skipstep1_b1_20_30_5e-05_5e-07_unique_sent.pt\"\n",
    "root = \"/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/datasets_eeg_text/zuco\"\n",
    "\n",
    "raw=\"/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/notebooks/\"\n",
    "config_path=\"task1_task2_taskNRv2_finetune_BrainTranslator_skipstep1_b32_20_30_5e-05_5e-07_unique_sent.json\"\n",
    "\n",
    "training_config = load_json_file(raw,config_path)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "subject_choice = training_config['subjects']\n",
    "print(f'[INFO]subjects: {subject_choice}')\n",
    "eeg_type_choice = training_config['eeg_type']\n",
    "print(f'[INFO]eeg type: {eeg_type_choice}')\n",
    "bands_choice = training_config['eeg_bands']\n",
    "print(f'[INFO]using bands: {bands_choice}')\n",
    "\n",
    "dataset_setting = 'unique_sent'\n",
    "\n",
    "task_name = training_config['task_name']\n",
    "\n",
    "model_name = training_config['model_name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]loading 3 task datasets\n",
      "[INFO]using subjects:  ['ZAB', 'ZDM', 'ZDN', 'ZGW', 'ZJM', 'ZJN', 'ZJS', 'ZKB', 'ZKH', 'ZKW', 'ZMG', 'ZPH']\n",
      "train divider = 320\n",
      "dev divider = 360\n",
      "[INFO]initializing a test set...\n",
      "++ adding task to dataset, now we have: 456\n",
      "[INFO]using subjects:  ['ZAB', 'ZDM', 'ZDN', 'ZGW', 'ZJM', 'ZJN', 'ZJS', 'ZKB', 'ZKH', 'ZKW', 'ZMG', 'ZPH']\n",
      "train divider = 240\n",
      "dev divider = 270\n",
      "[INFO]initializing a test set...\n",
      "discard length zero instance:  He was the son of a blacksmith Timothy Bush, Jr. and Lydia Newcomb and was born in Penfield, Monroe Co., New York on January 28, 1797.\n",
      "discard length zero instance:  Mary Lilian Baels (November 28, 1916 - June 7, 2002) was best known as Princess de Ruthy, the controversial morganatic second wife of King Leopold III of the Belgians.\n",
      "++ adding task to dataset, now we have: 806\n",
      "[INFO]using subjects:  ['YFS', 'YHS', 'YRP', 'YIS', 'YAK', 'YRH', 'YFR', 'YDG', 'YMS', 'YAG', 'YRK', 'YSL', 'YLS', 'YDR', 'YAC', 'YTL', 'YMD', 'YSD']\n",
      "train divider = 279\n",
      "dev divider = 313\n",
      "[INFO]initializing a test set...\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "expect word eeg embedding dim to be 840, but got 0, return None\n",
      "++ adding task to dataset, now we have: 1407\n",
      "[INFO]input tensor size: torch.Size([56, 840])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets_loaded=load_datasets(['task1','task2','taskNRv2'],root)\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "test_set = ZuCo_dataset(datasets_loaded,\n",
    "                         'test',\n",
    "                           tokenizer,\n",
    "                             subject = subject_choice,\n",
    "                               eeg_type = eeg_type_choice,\n",
    "                                 bands = bands_choice,\n",
    "                                   setting = dataset_setting)\n",
    "\n",
    "test_dataloader = DataLoader(test_set, batch_size = 1, shuffle=False, num_workers=0)\n",
    "\n",
    "dataloaders = {'test':test_dataloader}\n",
    "\n",
    "pretrained_bart = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" main architecture for open vocabulary EEG-To-Text decoding\"\"\"\n",
    "class BrainTranslator(nn.Module):\n",
    "    def __init__(self, pretrained_layers, in_feature = 840, decoder_embedding_size = 1024, additional_encoder_nhead=8, additional_encoder_dim_feedforward = 2048):\n",
    "        super(BrainTranslator, self).__init__()\n",
    "        \n",
    "        self.pretrained = pretrained_layers\n",
    "        # additional transformer encoder, following BART paper about \n",
    "        self.additional_encoder_layer = nn.TransformerEncoderLayer(d_model=in_feature, nhead=additional_encoder_nhead,  dim_feedforward = additional_encoder_dim_feedforward, batch_first=True)\n",
    "        self.additional_encoder = nn.TransformerEncoder(self.additional_encoder_layer, num_layers=6)\n",
    "        \n",
    "        # print('[INFO]adding positional embedding')\n",
    "        # self.positional_embedding = PositionalEncoding(in_feature)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_feature, decoder_embedding_size)\n",
    "\n",
    "    def forward(self, input_embeddings_batch, input_masks_batch, input_masks_invert, target_ids_batch_converted):\n",
    "        \"\"\"input_embeddings_batch: batch_size*Seq_len*840\"\"\"\n",
    "        \"\"\"input_mask: 1 is not masked, 0 is masked\"\"\"\n",
    "        \"\"\"input_masks_invert: 1 is masked, 0 is not masked\"\"\"\n",
    "        \n",
    "        # input_embeddings_batch = self.positional_embedding(input_embeddings_batch) \n",
    "\n",
    "        # use src_key_padding_masks\n",
    "        encoded_embedding = self.additional_encoder(input_embeddings_batch, src_key_padding_mask = input_masks_invert) \n",
    "        \n",
    "        # encoded_embedding = self.additional_encoder(input_embeddings_batch) \n",
    "        encoded_embedding = F.relu(self.fc1(encoded_embedding))\n",
    "        out = self.pretrained(inputs_embeds = encoded_embedding, attention_mask = input_masks_batch, return_dict = True, labels = target_ids_batch_converted)                    \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BrainTranslator(\n",
       "  (pretrained): BartForConditionalGeneration(\n",
       "    (model): BartModel(\n",
       "      (shared): Embedding(50265, 1024, padding_idx=1)\n",
       "      (encoder): BartEncoder(\n",
       "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): BartDecoder(\n",
       "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
       "  )\n",
       "  (additional_encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=840, out_features=840, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=840, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=840, bias=True)\n",
       "    (norm1): LayerNorm((840,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((840,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (additional_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=840, out_features=840, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=840, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=840, bias=True)\n",
       "        (norm1): LayerNorm((840,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((840,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=840, out_features=1024, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BrainTranslator(pretrained_bart,\n",
    "                          in_feature = 105*len(bands_choice),\n",
    "                            decoder_embedding_size = 1024,\n",
    "                              additional_encoder_nhead=8,\n",
    "                                additional_encoder_dim_feedforward = 2048)\n",
    "model_weights = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(model_weights)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_other(dataloaders, device, tokenizer, criterion, model, output_all_results_path='./results/temp.txt'):\n",
    "    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "    pred_tokens_list = []\n",
    "    pred_string_list = []\n",
    "\n",
    "    batch_counter = 0  # Add a counter for the batches\n",
    "\n",
    "    with open(output_all_results_path, 'w') as f:\n",
    "        for input_embeddings, seq_len, input_masks, input_mask_invert, target_ids, target_mask, sentiment_labels, sent_level_EEG in dataloaders['test']:\n",
    "            \n",
    "            # Check if we have processed 40 batches\n",
    "            if batch_counter >= 40:\n",
    "                break\n",
    "            \n",
    "            input_embeddings_batch = input_embeddings.to(device).float()\n",
    "            input_masks_batch = input_masks.to(device)\n",
    "            target_ids_batch = target_ids.to(device)\n",
    "            input_mask_invert_batch = input_mask_invert.to(device)\n",
    "\n",
    "            # replace padding ids in target_ids with -100\n",
    "            target_ids_batch[target_ids_batch == tokenizer.pad_token_id] = -100 \n",
    "\n",
    "            # forward\n",
    "            seq2seqLMoutput = model(input_embeddings_batch, input_masks_batch, input_mask_invert_batch, target_ids_batch)\n",
    "\n",
    "            # get predicted tokens\n",
    "            logits = seq2seqLMoutput.logits\n",
    "            probs = logits[0].softmax(dim=1)\n",
    "            values, predictions = probs.topk(1)\n",
    "            predictions = torch.squeeze(predictions)\n",
    "            predicted_string = tokenizer.decode(predictions).split('</s></s>')[0].replace('<s>', '')\n",
    "            f.write(f'predicted string: {predicted_string}\\n')\n",
    "            \n",
    "            # convert to int list\n",
    "            predictions = predictions.tolist()\n",
    "            truncated_prediction = []\n",
    "            for t in predictions:\n",
    "                if t != tokenizer.eos_token_id:\n",
    "                    truncated_prediction.append(t)\n",
    "                else:\n",
    "                    break\n",
    "            pred_tokens = tokenizer.convert_ids_to_tokens(truncated_prediction, skip_special_tokens=True)\n",
    "            pred_tokens_list.append(pred_tokens)\n",
    "            pred_string_list.append(predicted_string)\n",
    "\n",
    "            batch_counter += 1  # Increment the batch counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = eval_model(dataloaders, device, tokenizer,criterion, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = next(iter(dataloaders['test']))\n",
    "print(len(first_batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Your training code here\n",
    "\n",
    "    strings =evaluate_model_simple(dataloaders, device, tokenizer,model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_model(dataloaders, device, tokenizer, criterion, model, output_all_results_path = './results/temp.txt' ):\n",
    "    # modified from: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "    model.eval()   # Set model to evaluate mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Iterate over data.\n",
    "    sample_count = 0\n",
    "    \n",
    "    target_tokens_list = []\n",
    "    target_string_list = []\n",
    "    pred_tokens_list = []\n",
    "    pred_string_list = []\n",
    "    with open(output_all_results_path,'w') as f:\n",
    "        for input_embeddings, seq_len, input_masks, input_mask_invert, target_ids, target_mask, sentiment_labels, sent_level_EEG in dataloaders['test']:\n",
    "            # load in batch\n",
    "            input_embeddings_batch = input_embeddings.to(device).float()\n",
    "            input_masks_batch = input_masks.to(device)\n",
    "            target_ids_batch = target_ids.to(device)\n",
    "            input_mask_invert_batch = input_mask_invert.to(device)\n",
    "            \n",
    "            target_tokens = tokenizer.convert_ids_to_tokens(target_ids_batch[0].tolist(), skip_special_tokens = True)\n",
    "            target_string = tokenizer.decode(target_ids_batch[0], skip_special_tokens = True)\n",
    "            # print('target ids tensor:',target_ids_batch[0])\n",
    "            # print('target ids:',target_ids_batch[0].tolist())\n",
    "            # print('target tokens:',target_tokens)\n",
    "            # print('target string:',target_string)\n",
    "            f.write(f'target string: {target_string}\\n')\n",
    "\n",
    "            # add to list for later calculate bleu metric\n",
    "            target_tokens_list.append([target_tokens])\n",
    "            target_string_list.append(target_string)\n",
    "            \n",
    "            \"\"\"replace padding ids in target_ids with -100\"\"\"\n",
    "            target_ids_batch[target_ids_batch == tokenizer.pad_token_id] = -100 \n",
    "\n",
    "            # target_ids_batch_label = target_ids_batch.clone().detach()\n",
    "            # target_ids_batch_label[target_ids_batch_label == tokenizer.pad_token_id] = -100\n",
    "\n",
    "            # forward\n",
    "            seq2seqLMoutput = model(input_embeddings_batch, input_masks_batch, input_mask_invert_batch, target_ids_batch)\n",
    "\n",
    "            \"\"\"calculate loss\"\"\"\n",
    "            # logits = seq2seqLMoutput.logits # 8*48*50265\n",
    "            # logits = logits.permute(0,2,1) # 8*50265*48\n",
    "\n",
    "            # loss = criterion(logits, target_ids_batch_label) # calculate cross entropy loss only on encoded target parts\n",
    "            # NOTE: my criterion not used\n",
    "            loss = seq2seqLMoutput.loss # use the BART language modeling loss\n",
    "\n",
    "\n",
    "            # get predicted tokens\n",
    "            # print('target size:', target_ids_batch.size(), ',original logits size:', logits.size())\n",
    "            logits = seq2seqLMoutput.logits # 8*48*50265\n",
    "            # logits = logits.permute(0,2,1)\n",
    "            # print('permuted logits size:', logits.size())\n",
    "            probs = logits[0].softmax(dim = 1)\n",
    "            # print('probs size:', probs.size())\n",
    "            values, predictions = probs.topk(1)\n",
    "            # print('predictions before squeeze:',predictions.size())\n",
    "            predictions = torch.squeeze(predictions)\n",
    "            predicted_string = tokenizer.decode(predictions).split('</s></s>')[0].replace('<s>','')\n",
    "            # print('predicted string:',predicted_string)\n",
    "            f.write(f'predicted string: {predicted_string}\\n')\n",
    "            f.write(f'################################################\\n\\n\\n')\n",
    "\n",
    "            # convert to int list\n",
    "            predictions = predictions.tolist()\n",
    "            truncated_prediction = []\n",
    "            for t in predictions:\n",
    "                if t != tokenizer.eos_token_id:\n",
    "                    truncated_prediction.append(t)\n",
    "                else:\n",
    "                    break\n",
    "            pred_tokens = tokenizer.convert_ids_to_tokens(truncated_prediction, skip_special_tokens = True)\n",
    "            # print('predicted tokens:',pred_tokens)\n",
    "            pred_tokens_list.append(pred_tokens)\n",
    "            pred_string_list.append(predicted_string)\n",
    "            # print('################################################')\n",
    "            # print()\n",
    "\n",
    "            sample_count += 1\n",
    "            # statistics\n",
    "            running_loss += loss.item() * input_embeddings_batch.size()[0] # batch loss\n",
    "            # print('[DEBUG]loss:',loss.item())\n",
    "            # print('#################################')\n",
    "\n",
    "\n",
    "    epoch_loss = running_loss / dataset_sizes['test_set']\n",
    "    print('test loss: {:4f}'.format(epoch_loss))\n",
    "\n",
    "    \"\"\" calculate corpus bleu score \"\"\"\n",
    "    weights_list = [(1.0,),(0.5,0.5),(1./3.,1./3.,1./3.),(0.25,0.25,0.25,0.25)]\n",
    "    for weight in weights_list:\n",
    "        # print('weight:',weight)\n",
    "        corpus_bleu_score = corpus_bleu(target_tokens_list, pred_tokens_list, weights = weight)\n",
    "        print(f'corpus BLEU-{len(list(weight))} score:', corpus_bleu_score)\n",
    "\n",
    "    print()\n",
    "    \"\"\" calculate rouge score \"\"\"\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = rouge.get_scores(pred_string_list,target_string_list, avg = True)\n",
    "    print(rouge_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
