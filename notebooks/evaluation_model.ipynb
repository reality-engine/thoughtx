{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # add parent directory to system path\n",
    "import torch\n",
    "import json\n",
    "from transformers import BartTokenizer\n",
    "from model.model_loader import get_model\n",
    "\n",
    "# Re-importing the necessary libraries\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_simple(dataloaders, device, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a given dataset and returns the predicted strings for inspection.\n",
    "    \n",
    "    Args:\n",
    "    - dataloaders (dict): A dictionary containing the dataloaders for different sets ('test' in this case).\n",
    "    - device (torch.device): The device to run the model on.\n",
    "    - tokenizer (transformers tokenizer): The tokenizer used for decoding the model outputs.\n",
    "    - model (nn.Module): The model to be evaluated.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list containing the predicted strings for inspection.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()  # Set model to evaluate mode\n",
    "    predicted_strings = []  # Store the predicted strings\n",
    "\n",
    "    # Process only 10 batches from the dataloader\n",
    "    for i, (input_embeddings, seq_len, input_masks, input_mask_invert, target_ids, _, _, _) in enumerate(dataloaders['test']):\n",
    "        if i >= 10:\n",
    "            break\n",
    "\n",
    "        # Transfer data to the specified device\n",
    "        input_embeddings_batch = input_embeddings.to(device).float()\n",
    "        input_masks_batch = input_masks.to(device)\n",
    "        target_ids_batch = target_ids.to(device)\n",
    "        input_mask_invert_batch = input_mask_invert.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        seq2seqLMoutput = model(input_embeddings_batch, input_masks_batch, input_mask_invert_batch, target_ids_batch)\n",
    "        logits = seq2seqLMoutput.logits\n",
    "        probs = logits[0].softmax(dim=1)\n",
    "        _, predictions = probs.topk(1)\n",
    "        predictions = torch.squeeze(predictions)\n",
    "        predicted_string = tokenizer.decode(predictions).split('</s></s>')[0].replace('<s>', '')\n",
    "        \n",
    "        # Append the predicted string to the list\n",
    "        predicted_strings.append(predicted_string)\n",
    "\n",
    "    return predicted_strings\n",
    "\n",
    "# This is the refactored function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.zuco_data import ZuCo_dataset, get_input_sample,evaluate_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw=\"/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/notebooks/\"\n",
    "config_path=\"task1_task2_taskNRv2_finetune_BrainTranslator_skipstep1_b32_20_30_5e-05_5e-07_unique_sent.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_json_file(raw, filename):\n",
    "    with open(raw + filename, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = load_json_file(raw,config_path)\n",
    "training_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "subject_choice = training_config['subjects']\n",
    "print(f'[INFO]subjects: {subject_choice}')\n",
    "eeg_type_choice = training_config['eeg_type']\n",
    "print(f'[INFO]eeg type: {eeg_type_choice}')\n",
    "bands_choice = training_config['eeg_bands']\n",
    "print(f'[INFO]using bands: {bands_choice}')\n",
    "\n",
    "dataset_setting = 'unique_sent'\n",
    "\n",
    "task_name = training_config['task_name']\n",
    "\n",
    "model_name = training_config['model_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root = \"/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/datasets_eeg_text/zuco\"\n",
    "\n",
    "def load_datasets(task_names):\n",
    "    \"\"\" Load datasets for the given tasks \"\"\"\n",
    "    task_paths = {\n",
    "        'task1': os.path.join(root, 'task1-SR/pickle/task1-SR-dataset.pickle'),\n",
    "        'task2': os.path.join(root, 'task2-NR/pickle/task2-NR-dataset.pickle'),\n",
    "        'task3': os.path.join(root, 'task3-TSR/pickle/task3-TSR-dataset.pickle'),\n",
    "        'taskNRv2': os.path.join(root, 'task2-NR-2.0/pickle/task2-NR-2.0-dataset.pickle')\n",
    "    }\n",
    "    \n",
    "    whole_dataset_dicts = []\n",
    "    for task_name in task_names:\n",
    "        if task_name in task_paths:\n",
    "            with open(task_paths[task_name], 'rb') as handle:\n",
    "                whole_dataset_dicts.append(pickle.load(handle))\n",
    "\n",
    "    return whole_dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_loaded=load_datasets(['task1','task2','taskNRv2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = ZuCo_dataset(datasets_loaded,\n",
    "                         'test',\n",
    "                           tokenizer,\n",
    "                             subject = subject_choice,\n",
    "                               eeg_type = eeg_type_choice,\n",
    "                                 bands = bands_choice,\n",
    "                                   setting = dataset_setting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_set, batch_size = 1, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {'test':test_dataloader}\n",
    "dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_bart = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=get_model()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" main architecture for open vocabulary EEG-To-Text decoding\"\"\"\n",
    "class BrainTranslator(nn.Module):\n",
    "    def __init__(self, pretrained_layers, in_feature = 840, decoder_embedding_size = 1024, additional_encoder_nhead=8, additional_encoder_dim_feedforward = 2048):\n",
    "        super(BrainTranslator, self).__init__()\n",
    "        \n",
    "        self.pretrained = pretrained_layers\n",
    "        # additional transformer encoder, following BART paper about \n",
    "        self.additional_encoder_layer = nn.TransformerEncoderLayer(d_model=in_feature, nhead=additional_encoder_nhead,  dim_feedforward = additional_encoder_dim_feedforward, batch_first=True)\n",
    "        self.additional_encoder = nn.TransformerEncoder(self.additional_encoder_layer, num_layers=6)\n",
    "        \n",
    "        # print('[INFO]adding positional embedding')\n",
    "        # self.positional_embedding = PositionalEncoding(in_feature)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_feature, decoder_embedding_size)\n",
    "\n",
    "    def forward(self, input_embeddings_batch, input_masks_batch, input_masks_invert, target_ids_batch_converted):\n",
    "        \"\"\"input_embeddings_batch: batch_size*Seq_len*840\"\"\"\n",
    "        \"\"\"input_mask: 1 is not masked, 0 is masked\"\"\"\n",
    "        \"\"\"input_masks_invert: 1 is masked, 0 is not masked\"\"\"\n",
    "        \n",
    "        # input_embeddings_batch = self.positional_embedding(input_embeddings_batch) \n",
    "\n",
    "        # use src_key_padding_masks\n",
    "        encoded_embedding = self.additional_encoder(input_embeddings_batch, src_key_padding_mask = input_masks_invert) \n",
    "        \n",
    "        # encoded_embedding = self.additional_encoder(input_embeddings_batch) \n",
    "        encoded_embedding = F.relu(self.fc1(encoded_embedding))\n",
    "        out = self.pretrained(inputs_embeds = encoded_embedding, attention_mask = input_masks_batch, return_dict = True, labels = target_ids_batch_converted)                    \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_bart = BartForConditionalGeneration.from_pretrained(\n",
    "            \"facebook/bart-large\"\n",
    "        )\n",
    "\n",
    "        # Use the correct path to your model weights\n",
    "checkpoint_path = \"/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/local_checkpoint/task1_task2_taskNRv2_finetune_BrainTranslator_skipstep1_b1_20_30_5e-05_5e-07_unique_sent.pt\"\n",
    "\n",
    "# Initialize BrainTranslator with the pretrained BART layers\n",
    "\n",
    "try:\n",
    "    model = BrainTranslator(pretrained_bart)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error initializing BrainTranslator: {str(e)}\")\n",
    "\n",
    "model_weights = torch.load(\n",
    "    checkpoint_path, map_location=torch.device(\"cpu\")\n",
    ")\n",
    "model.load_state_dict(model_weights)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BrainTranslator(pretrained_bart,\n",
    "#                           in_feature = 105*len(bands_choice),\n",
    "#                             decoder_embedding_size = 1024,\n",
    "#                               additional_encoder_nhead=8,\n",
    "#                                 additional_encoder_dim_feedforward = 2048)\n",
    "\n",
    "# model.load_state_dict(torch.load(checkpoint_path))\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_all_results_path= \"./results\"\n",
    "\n",
    "output_all_results_path = f'./results/{task_name}-{model_name}-all_decoding_results.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(dataloaders, device, tokenizer, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = next(iter(dataloaders['test']))\n",
    "print(len(first_batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Your training code here\n",
    "\n",
    "    strings =evaluate_model_simple(dataloaders, device, tokenizer,model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_model(dataloaders, device, tokenizer, criterion, model, output_all_results_path = './results/temp.txt' ):\n",
    "    # modified from: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "    model.eval()   # Set model to evaluate mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Iterate over data.\n",
    "    sample_count = 0\n",
    "    \n",
    "    target_tokens_list = []\n",
    "    target_string_list = []\n",
    "    pred_tokens_list = []\n",
    "    pred_string_list = []\n",
    "    with open(output_all_results_path,'w') as f:\n",
    "        for input_embeddings, seq_len, input_masks, input_mask_invert, target_ids, target_mask, sentiment_labels, sent_level_EEG in dataloaders['test']:\n",
    "            # load in batch\n",
    "            input_embeddings_batch = input_embeddings.to(device).float()\n",
    "            input_masks_batch = input_masks.to(device)\n",
    "            target_ids_batch = target_ids.to(device)\n",
    "            input_mask_invert_batch = input_mask_invert.to(device)\n",
    "            \n",
    "            target_tokens = tokenizer.convert_ids_to_tokens(target_ids_batch[0].tolist(), skip_special_tokens = True)\n",
    "            target_string = tokenizer.decode(target_ids_batch[0], skip_special_tokens = True)\n",
    "            # print('target ids tensor:',target_ids_batch[0])\n",
    "            # print('target ids:',target_ids_batch[0].tolist())\n",
    "            # print('target tokens:',target_tokens)\n",
    "            # print('target string:',target_string)\n",
    "            f.write(f'target string: {target_string}\\n')\n",
    "\n",
    "            # add to list for later calculate bleu metric\n",
    "            target_tokens_list.append([target_tokens])\n",
    "            target_string_list.append(target_string)\n",
    "            \n",
    "            \"\"\"replace padding ids in target_ids with -100\"\"\"\n",
    "            target_ids_batch[target_ids_batch == tokenizer.pad_token_id] = -100 \n",
    "\n",
    "            # target_ids_batch_label = target_ids_batch.clone().detach()\n",
    "            # target_ids_batch_label[target_ids_batch_label == tokenizer.pad_token_id] = -100\n",
    "\n",
    "            # forward\n",
    "            seq2seqLMoutput = model(input_embeddings_batch, input_masks_batch, input_mask_invert_batch, target_ids_batch)\n",
    "\n",
    "            \"\"\"calculate loss\"\"\"\n",
    "            # logits = seq2seqLMoutput.logits # 8*48*50265\n",
    "            # logits = logits.permute(0,2,1) # 8*50265*48\n",
    "\n",
    "            # loss = criterion(logits, target_ids_batch_label) # calculate cross entropy loss only on encoded target parts\n",
    "            # NOTE: my criterion not used\n",
    "            loss = seq2seqLMoutput.loss # use the BART language modeling loss\n",
    "\n",
    "\n",
    "            # get predicted tokens\n",
    "            # print('target size:', target_ids_batch.size(), ',original logits size:', logits.size())\n",
    "            logits = seq2seqLMoutput.logits # 8*48*50265\n",
    "            # logits = logits.permute(0,2,1)\n",
    "            # print('permuted logits size:', logits.size())\n",
    "            probs = logits[0].softmax(dim = 1)\n",
    "            # print('probs size:', probs.size())\n",
    "            values, predictions = probs.topk(1)\n",
    "            # print('predictions before squeeze:',predictions.size())\n",
    "            predictions = torch.squeeze(predictions)\n",
    "            predicted_string = tokenizer.decode(predictions).split('</s></s>')[0].replace('<s>','')\n",
    "            # print('predicted string:',predicted_string)\n",
    "            f.write(f'predicted string: {predicted_string}\\n')\n",
    "            f.write(f'################################################\\n\\n\\n')\n",
    "\n",
    "            # convert to int list\n",
    "            predictions = predictions.tolist()\n",
    "            truncated_prediction = []\n",
    "            for t in predictions:\n",
    "                if t != tokenizer.eos_token_id:\n",
    "                    truncated_prediction.append(t)\n",
    "                else:\n",
    "                    break\n",
    "            pred_tokens = tokenizer.convert_ids_to_tokens(truncated_prediction, skip_special_tokens = True)\n",
    "            # print('predicted tokens:',pred_tokens)\n",
    "            pred_tokens_list.append(pred_tokens)\n",
    "            pred_string_list.append(predicted_string)\n",
    "            # print('################################################')\n",
    "            # print()\n",
    "\n",
    "            sample_count += 1\n",
    "            # statistics\n",
    "            running_loss += loss.item() * input_embeddings_batch.size()[0] # batch loss\n",
    "            # print('[DEBUG]loss:',loss.item())\n",
    "            # print('#################################')\n",
    "\n",
    "\n",
    "    epoch_loss = running_loss / dataset_sizes['test_set']\n",
    "    print('test loss: {:4f}'.format(epoch_loss))\n",
    "\n",
    "    \"\"\" calculate corpus bleu score \"\"\"\n",
    "    weights_list = [(1.0,),(0.5,0.5),(1./3.,1./3.,1./3.),(0.25,0.25,0.25,0.25)]\n",
    "    for weight in weights_list:\n",
    "        # print('weight:',weight)\n",
    "        corpus_bleu_score = corpus_bleu(target_tokens_list, pred_tokens_list, weights = weight)\n",
    "        print(f'corpus BLEU-{len(list(weight))} score:', corpus_bleu_score)\n",
    "\n",
    "    print()\n",
    "    \"\"\" calculate rouge score \"\"\"\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = rouge.get_scores(pred_string_list,target_string_list, avg = True)\n",
    "    print(rouge_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
