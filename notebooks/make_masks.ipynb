{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in /opt/homebrew/lib/python3.11/site-packages (0.103.2)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: torch in /opt/homebrew/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: transformers in /opt/homebrew/lib/python3.11/site-packages (4.34.0)\n",
      "Requirement already satisfied: uvicorn in /opt/homebrew/lib/python3.11/site-packages (0.23.2)\n",
      "Requirement already satisfied: google-cloud-storage in /opt/homebrew/lib/python3.11/site-packages (2.11.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.11/site-packages (1.3.1)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /opt/homebrew/lib/python3.11/site-packages (from fastapi) (3.7.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /opt/homebrew/lib/python3.11/site-packages (from fastapi) (1.10.11)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/homebrew/lib/python3.11/site-packages (from fastapi) (0.27.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/homebrew/lib/python3.11/site-packages (from fastapi) (4.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michaelholborn/Library/Python/3.11/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (1.25.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/michaelholborn/Library/Python/3.11/lib/python/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/homebrew/lib/python3.11/site-packages (from uvicorn) (8.1.4)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/homebrew/lib/python3.11/site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/homebrew/lib/python3.11/site-packages (from google-cloud-storage) (2.23.3)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/homebrew/lib/python3.11/site-packages (from google-cloud-storage) (2.12.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/homebrew/lib/python3.11/site-packages (from google-cloud-storage) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/homebrew/lib/python3.11/site-packages (from google-cloud-storage) (2.6.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/lib/python3.11/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/lib/python3.11/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/homebrew/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.60.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /opt/homebrew/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (4.24.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/homebrew/lib/python3.11/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/homebrew/lib/python3.11/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.9)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from google-resumable-media>=2.6.0->google-cloud-storage) (1.5.0)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/michaelholborn/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/homebrew/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install fastapi pandas torch transformers uvicorn google-cloud-storage scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fastapi import FastAPI, UploadFile, HTTPException\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import io\n",
    "from transformers import BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "_MODEL = None\n",
    "\n",
    "def get_model():\n",
    "    global _MODEL\n",
    "\n",
    "    if _MODEL is None:\n",
    "        pretrained_bart = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "        checkpoint_path = '/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/task1_task2_taskNRv2_finetune_BrainTranslator_skipstep1_b1_20_30_5e-05_5e-07_unique_sent.pt'  # Change to the path of your model\n",
    "        _MODEL = BrainTranslator(pretrained_bart)\n",
    "        model_weights = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "        _MODEL.load_state_dict(model_weights)\n",
    "        _MODEL.eval()\n",
    "\n",
    "    return _MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the BrainTranslator model\n",
    "class BrainTranslator(nn.Module):\n",
    "    def __init__(self, pretrained_layers, in_feature=840, decoder_embedding_size=1024, additional_encoder_nhead=8, additional_encoder_dim_feedforward=2048):\n",
    "        super(BrainTranslator, self).__init__()\n",
    "        \n",
    "        self.pretrained = pretrained_layers\n",
    "        self.additional_encoder_layer = nn.TransformerEncoderLayer(d_model=in_feature, nhead=additional_encoder_nhead, dim_feedforward=additional_encoder_dim_feedforward, batch_first=True)\n",
    "        self.additional_encoder = nn.TransformerEncoder(self.additional_encoder_layer, num_layers=6)\n",
    "        self.fc1 = nn.Linear(in_feature, decoder_embedding_size)\n",
    "\n",
    "    def forward(self, input_embeddings_batch, input_masks_batch, input_mask_invert, decoder_input_ids):\n",
    "        encoded_embedding = self.additional_encoder(input_embeddings_batch, src_key_padding_mask=input_mask_invert)\n",
    "        encoded_embedding = F.relu(self.fc1(encoded_embedding))\n",
    "        out = self.pretrained(inputs_embeds=encoded_embedding, attention_mask=input_masks_batch, decoder_input_ids=decoder_input_ids, return_dict=True)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mask our own masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def load_embeddings_from_file(filepath: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Load embeddings from a given JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - filepath (str): The path to the JSON file containing embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: A tensor containing the loaded embeddings.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r') as file:\n",
    "        embeddings_data = json.load(file)\n",
    "    return torch.tensor(embeddings_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings_data=load_embeddings_from_file(\"../datasets/input_old_model/us-central1_eeg-test_dodadqada_dodadqada_saved_data_input_embeddings_12.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_masks_from_embeddings(embeddings: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    Generate attention masks and their inverse for a given embeddings tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings (torch.Tensor): The embeddings tensor.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the attention mask and its inverse.\n",
    "    \"\"\"\n",
    "    # Assuming non-zero embeddings represent valid tokens and zeros represent padding\n",
    "    attn_mask = (embeddings.sum(dim=-1) != 0).float()\n",
    "    attn_mask_invert = 1.0 - attn_mask\n",
    "    return attn_mask, attn_mask_invert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask,attn_mask_invert=generate_masks_from_embeddings(input_embeddings_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8m/y9dvzjx95lg__pqq1cypt21h0000gn/T/ipykernel_31465/2687417766.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_embeddings_tensor = torch.tensor(input_embeddings_data)\n",
      "/var/folders/8m/y9dvzjx95lg__pqq1cypt21h0000gn/T/ipykernel_31465/2687417766.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_masks_tensor = torch.tensor(attn_mask)\n",
      "/var/folders/8m/y9dvzjx95lg__pqq1cypt21h0000gn/T/ipykernel_31465/2687417766.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_mask_invert_tensor = torch.tensor(attn_mask_invert)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 56, 840]), torch.Size([1, 56]), torch.Size([1, 56]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# Convert loaded data to PyTorch tensors\n",
    "input_embeddings_tensor = torch.tensor(input_embeddings_data)\n",
    "input_masks_tensor = torch.tensor(attn_mask)\n",
    "input_mask_invert_tensor = torch.tensor(attn_mask_invert)\n",
    "\n",
    "input_embeddings_tensor.shape, input_masks_tensor.shape, input_mask_invert_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "pretrained_bart = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "\n",
    "checkpoint_path = '/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/local_checkpoint/us-central1_eeg-test_dodadqada_eeg-text_eeg-text_checkpoints_decoding_best_task1_task2_taskNRv2_finetune_BrainTranslator_skipstep1_b32_20_30_5e-05_5e-07_unique_sent.pt'  # Change to the path of your model\n",
    "model = BrainTranslator(pretrained_bart)\n",
    "model_weights = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(model_weights)\n",
    "model.eval()\n",
    "\n",
    "# Create a placeholder token\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "placeholder_token = tokenizer(\"<s>\", return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._transformer_encoder_layer_fwd(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Attempt a forward pass\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        outputs = model(input_embeddings_tensor, input_masks_tensor, input_mask_invert_tensor, placeholder_token[\"input_ids\"])\n",
    "        result = \"Forward pass successful!\"\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "\n",
    "result\n",
    "outputs\n",
    "# Extract the generated token IDs from the model's outputs\n",
    "generated_ids = outputs.logits.argmax(dim=-1)\n",
    "generated_ids\n",
    "\n",
    "# Decode the token IDs to text\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' there'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result\n",
    "outputs\n",
    "# Extract the generated token IDs from the model's outputs\n",
    "generated_ids = outputs.logits.argmax(dim=-1)\n",
    "generated_ids\n",
    "\n",
    "# Decode the token IDs to text\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fastapi import FastAPI, UploadFile, HTTPException\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import io\n",
    "from transformers import BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "_MODEL = None\n",
    "\n",
    "def get_model():\n",
    "    global _MODEL\n",
    "\n",
    "    if _MODEL is None:\n",
    "        pretrained_bart = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "        checkpoint_path = '/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/task1_task2_taskNRv2_finetune_BrainTranslator_skipstep1_b1_20_30_5e-05_5e-07_unique_sent.pt'  # Change to the path of your model\n",
    "        _MODEL = BrainTranslator(pretrained_bart)\n",
    "        model_weights = torch.load(checkpoint_path, map_location=device)\n",
    "        _MODEL.load_state_dict(model_weights)\n",
    "        _MODEL.eval()\n",
    "\n",
    "    return _MODEL\n",
    "\n",
    "\n",
    "def load_embeddings_from_file(filepath: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Load embeddings from a given JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - filepath (str): The path to the JSON file containing embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: A tensor containing the loaded embeddings.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r') as file:\n",
    "        embeddings_data = json.load(file)\n",
    "    return torch.tensor(embeddings_data)\n",
    "\n",
    "def generate_masks_from_embeddings(embeddings: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    Generate attention masks and their inverse for a given embeddings tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings (torch.Tensor): The embeddings tensor.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the attention mask and its inverse.\n",
    "    \"\"\"\n",
    "    # Assuming non-zero embeddings represent valid tokens and zeros represent padding\n",
    "    attn_mask = (embeddings.sum(dim=-1) != 0).float()\n",
    "    attn_mask_invert = 1.0 - attn_mask\n",
    "    return attn_mask, attn_mask_invert\n",
    "\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\"\"\" main architecture for open vocabulary EEG-To-Text decoding\"\"\"\n",
    "\n",
    "# Define the BrainTranslator model\n",
    "# class BrainTranslator(nn.Module):\n",
    "    # def __init__(self, pretrained_layers, in_feature=840, decoder_embedding_size=1024, additional_encoder_nhead=8, additional_encoder_dim_feedforward=2048):\n",
    "    #     super(BrainTranslator, self).__init__()\n",
    "        \n",
    "    #     self.pretrained = pretrained_layers\n",
    "    #     self.additional_encoder_layer = nn.TransformerEncoderLayer(d_model=in_feature, nhead=additional_encoder_nhead, dim_feedforward=additional_encoder_dim_feedforward, batch_first=True)\n",
    "    #     self.additional_encoder = nn.TransformerEncoder(self.additional_encoder_layer, num_layers=6)\n",
    "    #     self.fc1 = nn.Linear(in_feature, decoder_embedding_size)\n",
    "\n",
    "    # def forward(self, input_embeddings_batch, input_masks_batch, input_mask_invert, decoder_input_ids):\n",
    "    #     encoded_embedding = self.additional_encoder(input_embeddings_batch, src_key_padding_mask=input_mask_invert)\n",
    "    #     encoded_embedding = F.relu(self.fc1(encoded_embedding))\n",
    "    #     out = self.pretrained(inputs_embeds=encoded_embedding, attention_mask=input_masks_batch, decoder_input_ids=decoder_input_ids, return_dict=True)\n",
    "    #     return out\n",
    "\n",
    "\n",
    "class BrainTranslator(nn.Module):\n",
    "    def __init__(self, pretrained_layers, in_feature = 840, decoder_embedding_size = 1024, additional_encoder_nhead=8, additional_encoder_dim_feedforward = 2048):\n",
    "        super(BrainTranslator, self).__init__()\n",
    "        \n",
    "        self.pretrained = pretrained_layers\n",
    "        # additional transformer encoder, following BART paper about \n",
    "        self.additional_encoder_layer = nn.TransformerEncoderLayer(d_model=in_feature, nhead=additional_encoder_nhead,  dim_feedforward = additional_encoder_dim_feedforward, batch_first=True)\n",
    "        self.additional_encoder = nn.TransformerEncoder(self.additional_encoder_layer, num_layers=6)\n",
    "        \n",
    "        # print('[INFO]adding positional embedding')\n",
    "        # self.positional_embedding = PositionalEncoding(in_feature)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_feature, decoder_embedding_size)\n",
    "\n",
    "    def forward(self, input_embeddings_batch, input_masks_batch, input_masks_invert, target_ids_batch_converted):\n",
    "        \"\"\"input_embeddings_batch: batch_size*Seq_len*840\"\"\"\n",
    "        \"\"\"input_mask: 1 is not masked, 0 is masked\"\"\"\n",
    "        \"\"\"input_masks_invert: 1 is masked, 0 is not masked\"\"\"\n",
    "        \n",
    "        # input_embeddings_batch = self.positional_embedding(input_embeddings_batch) \n",
    "\n",
    "        # use src_key_padding_masks\n",
    "        encoded_embedding = self.additional_encoder(input_embeddings_batch, src_key_padding_mask = input_masks_invert) \n",
    "        \n",
    "        # encoded_embedding = self.additional_encoder(input_embeddings_batch) \n",
    "        encoded_embedding = F.relu(self.fc1(encoded_embedding))\n",
    "        out = self.pretrained(inputs_embeds = encoded_embedding, attention_mask = input_masks_batch, return_dict = True, labels = target_ids_batch_converted)                    \n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "pretrained_bart = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "model = BrainTranslator(pretrained_bart)\n",
    "\n",
    "\n",
    "# model = BrainTranslator(pretrained_bart, in_feature = 105*len([\n",
    "#         \"_t1\",\n",
    "#         \"_t2\",\n",
    "#         \"_a1\",\n",
    "#         \"_a2\",\n",
    "#         \"_b1\",\n",
    "#         \"_b2\",\n",
    "#         \"_g1\",\n",
    "#         \"_g2\"\n",
    "#     ]), decoder_embedding_size = 1024, additional_encoder_nhead=8, additional_encoder_dim_feedforward = 2048)\n",
    "\n",
    "\n",
    "# Create a placeholder token\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "placeholder_token = tokenizer(\"<s>\", return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "pretrained_bart = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "\n",
    "checkpoint_path = '/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/local_checkpoint/us-central1_eeg-test_dodadqada_eeg-text_eeg-text_checkpoints_decoding_best_task1_task2_taskNRv2_finetune_BrainTranslator_skipstep1_b32_20_30_5e-05_5e-07_unique_sent.pt'  # Change to the path of your model\n",
    "model = BrainTranslator(pretrained_bart)\n",
    "model_weights = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(model_weights)\n",
    "model.eval()\n",
    "\n",
    "# Create a placeholder token\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "placeholder_token = tokenizer(\"<s>\", return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_1.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_1.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_2.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_2.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_3.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_3.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_4.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_4.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_5.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_5.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_6.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_6.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_7.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_7.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_8.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_8.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_9.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_9.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_10.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_10.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_11.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_11.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_12.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_12.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_13.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_13.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_14.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_14.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_15.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_15.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_16.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_16.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_17.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_17.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_18.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_18.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_19.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_19.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_20.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_20.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_21.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_21.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_22.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_22.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_23.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_23.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_24.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_24.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_25.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_25.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_26.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_26.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_27.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_27.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_28.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_28.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_29.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_29.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_30.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_30.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_31.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_31.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_32.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_32.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_33.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_33.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_34.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_34.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_35.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_35.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_36.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_36.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_37.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_37.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_38.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_38.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_39.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_39.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_40.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_40.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_41.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_41.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_42.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_42.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_43.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_43.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_44.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_44.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_45.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_45.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_46.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_46.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_47.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_47.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_48.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_48.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_49.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_49.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_50.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_50.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_51.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_51.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_52.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_52.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_53.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_53.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_54.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_54.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_55.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_55.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_56.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_56.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_57.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_57.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_58.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_58.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_59.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_59.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_60.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_60.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_61.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_61.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_62.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_62.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_63.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_63.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_64.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_64.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_65.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_65.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_66.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_66.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_67.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_67.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_68.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_68.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_69.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_69.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_70.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_70.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_71.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_71.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_72.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_72.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_73.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_73.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_74.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_74.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_75.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_75.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_76.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_76.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_77.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_77.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_78.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_78.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_79.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_79.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_80.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_80.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_81.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_81.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_82.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_82.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_83.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_83.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_84.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_84.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_85.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_85.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_86.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_86.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_87.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_87.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_88.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_88.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_89.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_89.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_90.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_90.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_91.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_91.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_92.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_92.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_93.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_93.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_94.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_94.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_95.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_95.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_96.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_96.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_97.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_97.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_98.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_98.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_99.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_99.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_100.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_100.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_101.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_101.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_102.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_102.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_103.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_103.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_104.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_104.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_105.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_105.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_106.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_106.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_107.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_107.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_108.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_108.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_109.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_109.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_110.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_110.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_111.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_111.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_112.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_112.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_113.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_113.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_114.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_114.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_115.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_115.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_116.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_116.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_117.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_117.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_118.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_118.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_119.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_119.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_120.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_120.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_121.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_121.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_122.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_122.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_123.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_123.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_124.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_124.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_125.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_125.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_126.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_126.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_127.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_127.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_128.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_128.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_129.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_129.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_130.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_130.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_131.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_131.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_132.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_132.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_133.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_133.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_134.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_134.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_135.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_135.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_136.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_136.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_137.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_137.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_138.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_138.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_139.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_139.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_140.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_140.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_141.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_141.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_142.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_142.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_143.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_143.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_144.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_144.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_145.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_145.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_146.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_146.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_147.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_147.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_148.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_148.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_149.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_149.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_150.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_150.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_151.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_151.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_152.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_152.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_153.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_153.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_154.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_154.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_155.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_155.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_156.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_156.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_157.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_157.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_158.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_158.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_159.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_159.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_160.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_160.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_161.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_161.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_162.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_162.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_163.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_163.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_164.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_164.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_165.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_165.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_166.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_166.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_167.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_167.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_168.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_168.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_169.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_169.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_170.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_170.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_171.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_171.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_172.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_172.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_173.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_173.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_174.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_174.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_175.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_175.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_176.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_176.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_177.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_177.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_178.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_178.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_179.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_179.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_180.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_180.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_181.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_181.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_182.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_182.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_183.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_183.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_184.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_184.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_185.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_185.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_186.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_186.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_187.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_187.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_188.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_188.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_189.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_189.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_190.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_190.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_191.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_191.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_192.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_192.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_193.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_193.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_194.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_194.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_195.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_195.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_196.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_196.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_197.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_197.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_198.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_198.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_199.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_199.json exists!\n",
      "/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_200.json\n",
      "File /Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_200.json exists!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "running_loss = 0.0\n",
    "\n",
    "# Iterate over data.\n",
    "sample_count = 0\n",
    "\n",
    "target_tokens_list = []\n",
    "target_string_list = []\n",
    "pred_tokens_list = []\n",
    "pred_string_list = []\n",
    "# Step 2: Create a loop to process each file\n",
    "results = []\n",
    "max_file_number=200\n",
    "for i in range(1, max_file_number + 1):\n",
    "    # Step 3: Dynamically build the file paths based on the loop index\n",
    "    input_embeddings_path = f\"/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/saved_data/input_embeddings_{i}.json\"\n",
    "    print(input_embeddings_path)\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(input_embeddings_path):\n",
    "        print(f\"File {input_embeddings_path} exists!\")\n",
    "    else:\n",
    "        print(f\"File {input_embeddings_path} does NOT exist!\")\n",
    "    # Step 4: Load the data from these files\n",
    "    try:\n",
    "       input_embeddings_data=load_embeddings_from_file(input_embeddings_path)\n",
    "       input_masks_data,input_mask_invert_data=generate_masks_from_embeddings(input_embeddings_data)\n",
    "    except FileNotFoundError:\n",
    "        # If one of the files isn't found, we'll append an error message and continue to the next iteration\n",
    "        results.append(f\"Files for index {i} not found.\")\n",
    "        continue\n",
    "\n",
    "    # Convert loaded data to PyTorch tensors\n",
    "    input_embeddings_tensor = input_embeddings_tensor.to(device).float()\n",
    "    input_masks_tensor = input_masks_tensor.to(device).float()\n",
    "    input_mask_invert_tensor = input_mask_invert_tensor.to(device).float()\n",
    "    \n",
    "    # Step 5: Process the data with the model\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            outputs = model(input_embeddings_tensor, input_masks_tensor, input_mask_invert_tensor, placeholder_token[\"input_ids\"])\n",
    "            # Extract the generated token IDs from the model's outputs\n",
    "            logits=outputs.logits\n",
    "            probs = logits[0].softmax(dim = 1)\n",
    "            values, predictions = probs.topk(1)\n",
    "            predictions = torch.squeeze(predictions)\n",
    "            predicted_string = tokenizer.decode(predictions).split('</s></s>')[0].replace('<s>','')\n",
    "            predictions = predictions.tolist()\n",
    "            truncated_prediction = []\n",
    "            for t in predictions:\n",
    "                if t != tokenizer.eos_token_id:\n",
    "                    truncated_prediction.append(t)\n",
    "                else:\n",
    "                    break\n",
    "            pred_tokens = tokenizer.convert_ids_to_tokens(truncated_prediction, skip_special_tokens = True)\n",
    "            # print('predicted tokens:',pred_tokens)\n",
    "            pred_tokens_list.append(pred_tokens)\n",
    "            pred_string_list.append(predicted_string)\n",
    "            print('predicted string:',predicted_string)\n",
    "            # results.append(generated_text)\n",
    "        except Exception as e:\n",
    "            results.append(str(e))\n",
    "\n",
    "pred_string_list[:10]  # Display the first 10 results for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained.final_logits_bias torch.Size([1, 50265])\n",
      "pretrained.model.shared.weight torch.Size([50265, 1024])\n",
      "pretrained.model.encoder.embed_tokens.weight torch.Size([50265, 1024])\n",
      "pretrained.model.encoder.embed_positions.weight torch.Size([1026, 1024])\n",
      "pretrained.model.encoder.layers.0.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.0.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.0.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.0.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.0.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.0.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.0.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.0.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.0.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.0.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.0.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.encoder.layers.0.fc1.bias torch.Size([4096])\n",
      "pretrained.model.encoder.layers.0.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.encoder.layers.0.fc2.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.0.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.0.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.1.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.1.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.1.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.1.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.1.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.1.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.1.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.1.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.1.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.1.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.1.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.encoder.layers.1.fc1.bias torch.Size([4096])\n",
      "pretrained.model.encoder.layers.1.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.encoder.layers.1.fc2.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.1.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.1.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.2.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.2.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.2.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.2.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.2.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.2.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.2.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.2.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.2.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.2.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.2.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.encoder.layers.2.fc1.bias torch.Size([4096])\n",
      "pretrained.model.encoder.layers.2.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.encoder.layers.2.fc2.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.2.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.2.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.3.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.3.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.3.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.3.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.3.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.3.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.3.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.3.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.3.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.3.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.3.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.encoder.layers.3.fc1.bias torch.Size([4096])\n",
      "pretrained.model.encoder.layers.3.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.encoder.layers.3.fc2.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.3.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.3.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.4.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.4.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.4.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.4.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.4.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.4.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.4.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.4.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.4.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.4.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.4.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.encoder.layers.4.fc1.bias torch.Size([4096])\n",
      "pretrained.model.encoder.layers.4.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.encoder.layers.4.fc2.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.4.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.4.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.5.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.5.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.5.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.5.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.5.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.5.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.5.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.5.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.5.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.5.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.5.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.encoder.layers.5.fc1.bias torch.Size([4096])\n",
      "pretrained.model.encoder.layers.5.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.encoder.layers.5.fc2.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.5.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.5.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.6.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.6.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.6.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.6.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.6.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.6.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.6.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.6.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.6.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.6.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.6.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.encoder.layers.6.fc1.bias torch.Size([4096])\n",
      "pretrained.model.encoder.layers.6.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.encoder.layers.6.fc2.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.6.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.6.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.7.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.7.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.7.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.7.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.7.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.7.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.7.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.7.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.7.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.7.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.7.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.encoder.layers.7.fc1.bias torch.Size([4096])\n",
      "pretrained.model.encoder.layers.7.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.encoder.layers.7.fc2.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.7.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.7.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.8.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.8.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.8.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.8.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.8.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.8.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.8.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.8.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.8.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.8.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.8.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.encoder.layers.8.fc1.bias torch.Size([4096])\n",
      "pretrained.model.encoder.layers.8.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.encoder.layers.8.fc2.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.8.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.8.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.9.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.9.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.9.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.9.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.9.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.9.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.9.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.9.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.9.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.9.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.9.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.encoder.layers.9.fc1.bias torch.Size([4096])\n",
      "pretrained.model.encoder.layers.9.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.encoder.layers.9.fc2.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.9.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.9.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.10.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.10.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.10.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.10.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.10.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.10.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.10.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.10.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.10.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.10.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.10.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.encoder.layers.10.fc1.bias torch.Size([4096])\n",
      "pretrained.model.encoder.layers.10.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.encoder.layers.10.fc2.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.10.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.10.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.11.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.11.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.11.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.11.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.11.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.11.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.11.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.11.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.11.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.11.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.11.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.encoder.layers.11.fc1.bias torch.Size([4096])\n",
      "pretrained.model.encoder.layers.11.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.encoder.layers.11.fc2.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.11.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.11.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layernorm_embedding.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layernorm_embedding.bias torch.Size([1024])\n",
      "pretrained.model.decoder.embed_tokens.weight torch.Size([50265, 1024])\n",
      "pretrained.model.decoder.embed_positions.weight torch.Size([1026, 1024])\n",
      "pretrained.model.decoder.layers.0.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.0.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.0.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.0.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.0.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.0.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.0.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.0.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.0.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.decoder.layers.0.fc1.bias torch.Size([4096])\n",
      "pretrained.model.decoder.layers.0.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.decoder.layers.0.fc2.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.1.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.1.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.1.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.1.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.1.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.1.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.1.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.1.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.decoder.layers.1.fc1.bias torch.Size([4096])\n",
      "pretrained.model.decoder.layers.1.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.decoder.layers.1.fc2.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.2.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.2.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.2.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.2.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.2.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.2.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.2.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.2.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.decoder.layers.2.fc1.bias torch.Size([4096])\n",
      "pretrained.model.decoder.layers.2.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.decoder.layers.2.fc2.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.3.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.3.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.3.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.3.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.3.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.3.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.3.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.3.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.decoder.layers.3.fc1.bias torch.Size([4096])\n",
      "pretrained.model.decoder.layers.3.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.decoder.layers.3.fc2.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.4.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.4.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.4.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.4.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.4.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.4.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.4.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.4.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.decoder.layers.4.fc1.bias torch.Size([4096])\n",
      "pretrained.model.decoder.layers.4.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.decoder.layers.4.fc2.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.5.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.5.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.5.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.5.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.5.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.5.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.5.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.5.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.decoder.layers.5.fc1.bias torch.Size([4096])\n",
      "pretrained.model.decoder.layers.5.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.decoder.layers.5.fc2.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.6.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.6.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.6.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.6.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.6.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.6.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.6.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.6.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.decoder.layers.6.fc1.bias torch.Size([4096])\n",
      "pretrained.model.decoder.layers.6.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.decoder.layers.6.fc2.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.7.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.7.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.7.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.7.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.7.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.7.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.7.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.7.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.decoder.layers.7.fc1.bias torch.Size([4096])\n",
      "pretrained.model.decoder.layers.7.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.decoder.layers.7.fc2.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.8.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.8.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.8.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.8.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.8.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.8.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.8.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.8.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.decoder.layers.8.fc1.bias torch.Size([4096])\n",
      "pretrained.model.decoder.layers.8.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.decoder.layers.8.fc2.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.9.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.9.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.9.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.9.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.9.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.9.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.9.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.9.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.decoder.layers.9.fc1.bias torch.Size([4096])\n",
      "pretrained.model.decoder.layers.9.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.decoder.layers.9.fc2.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.10.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.10.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.10.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.10.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.10.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.10.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.10.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.10.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.decoder.layers.10.fc1.bias torch.Size([4096])\n",
      "pretrained.model.decoder.layers.10.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.decoder.layers.10.fc2.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.11.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.11.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.11.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.11.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.11.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.11.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.11.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.11.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.decoder.layers.11.fc1.bias torch.Size([4096])\n",
      "pretrained.model.decoder.layers.11.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.decoder.layers.11.fc2.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layernorm_embedding.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layernorm_embedding.bias torch.Size([1024])\n",
      "pretrained.lm_head.weight torch.Size([50265, 1024])\n",
      "additional_encoder_layer.self_attn.in_proj_weight torch.Size([2520, 840])\n",
      "additional_encoder_layer.self_attn.in_proj_bias torch.Size([2520])\n",
      "additional_encoder_layer.self_attn.out_proj.weight torch.Size([840, 840])\n",
      "additional_encoder_layer.self_attn.out_proj.bias torch.Size([840])\n",
      "additional_encoder_layer.linear1.weight torch.Size([2048, 840])\n",
      "additional_encoder_layer.linear1.bias torch.Size([2048])\n",
      "additional_encoder_layer.linear2.weight torch.Size([840, 2048])\n",
      "additional_encoder_layer.linear2.bias torch.Size([840])\n",
      "additional_encoder_layer.norm1.weight torch.Size([840])\n",
      "additional_encoder_layer.norm1.bias torch.Size([840])\n",
      "additional_encoder_layer.norm2.weight torch.Size([840])\n",
      "additional_encoder_layer.norm2.bias torch.Size([840])\n",
      "additional_encoder.layers.0.self_attn.in_proj_weight torch.Size([2520, 840])\n",
      "additional_encoder.layers.0.self_attn.in_proj_bias torch.Size([2520])\n",
      "additional_encoder.layers.0.self_attn.out_proj.weight torch.Size([840, 840])\n",
      "additional_encoder.layers.0.self_attn.out_proj.bias torch.Size([840])\n",
      "additional_encoder.layers.0.linear1.weight torch.Size([2048, 840])\n",
      "additional_encoder.layers.0.linear1.bias torch.Size([2048])\n",
      "additional_encoder.layers.0.linear2.weight torch.Size([840, 2048])\n",
      "additional_encoder.layers.0.linear2.bias torch.Size([840])\n",
      "additional_encoder.layers.0.norm1.weight torch.Size([840])\n",
      "additional_encoder.layers.0.norm1.bias torch.Size([840])\n",
      "additional_encoder.layers.0.norm2.weight torch.Size([840])\n",
      "additional_encoder.layers.0.norm2.bias torch.Size([840])\n",
      "additional_encoder.layers.1.self_attn.in_proj_weight torch.Size([2520, 840])\n",
      "additional_encoder.layers.1.self_attn.in_proj_bias torch.Size([2520])\n",
      "additional_encoder.layers.1.self_attn.out_proj.weight torch.Size([840, 840])\n",
      "additional_encoder.layers.1.self_attn.out_proj.bias torch.Size([840])\n",
      "additional_encoder.layers.1.linear1.weight torch.Size([2048, 840])\n",
      "additional_encoder.layers.1.linear1.bias torch.Size([2048])\n",
      "additional_encoder.layers.1.linear2.weight torch.Size([840, 2048])\n",
      "additional_encoder.layers.1.linear2.bias torch.Size([840])\n",
      "additional_encoder.layers.1.norm1.weight torch.Size([840])\n",
      "additional_encoder.layers.1.norm1.bias torch.Size([840])\n",
      "additional_encoder.layers.1.norm2.weight torch.Size([840])\n",
      "additional_encoder.layers.1.norm2.bias torch.Size([840])\n",
      "additional_encoder.layers.2.self_attn.in_proj_weight torch.Size([2520, 840])\n",
      "additional_encoder.layers.2.self_attn.in_proj_bias torch.Size([2520])\n",
      "additional_encoder.layers.2.self_attn.out_proj.weight torch.Size([840, 840])\n",
      "additional_encoder.layers.2.self_attn.out_proj.bias torch.Size([840])\n",
      "additional_encoder.layers.2.linear1.weight torch.Size([2048, 840])\n",
      "additional_encoder.layers.2.linear1.bias torch.Size([2048])\n",
      "additional_encoder.layers.2.linear2.weight torch.Size([840, 2048])\n",
      "additional_encoder.layers.2.linear2.bias torch.Size([840])\n",
      "additional_encoder.layers.2.norm1.weight torch.Size([840])\n",
      "additional_encoder.layers.2.norm1.bias torch.Size([840])\n",
      "additional_encoder.layers.2.norm2.weight torch.Size([840])\n",
      "additional_encoder.layers.2.norm2.bias torch.Size([840])\n",
      "additional_encoder.layers.3.self_attn.in_proj_weight torch.Size([2520, 840])\n",
      "additional_encoder.layers.3.self_attn.in_proj_bias torch.Size([2520])\n",
      "additional_encoder.layers.3.self_attn.out_proj.weight torch.Size([840, 840])\n",
      "additional_encoder.layers.3.self_attn.out_proj.bias torch.Size([840])\n",
      "additional_encoder.layers.3.linear1.weight torch.Size([2048, 840])\n",
      "additional_encoder.layers.3.linear1.bias torch.Size([2048])\n",
      "additional_encoder.layers.3.linear2.weight torch.Size([840, 2048])\n",
      "additional_encoder.layers.3.linear2.bias torch.Size([840])\n",
      "additional_encoder.layers.3.norm1.weight torch.Size([840])\n",
      "additional_encoder.layers.3.norm1.bias torch.Size([840])\n",
      "additional_encoder.layers.3.norm2.weight torch.Size([840])\n",
      "additional_encoder.layers.3.norm2.bias torch.Size([840])\n",
      "additional_encoder.layers.4.self_attn.in_proj_weight torch.Size([2520, 840])\n",
      "additional_encoder.layers.4.self_attn.in_proj_bias torch.Size([2520])\n",
      "additional_encoder.layers.4.self_attn.out_proj.weight torch.Size([840, 840])\n",
      "additional_encoder.layers.4.self_attn.out_proj.bias torch.Size([840])\n",
      "additional_encoder.layers.4.linear1.weight torch.Size([2048, 840])\n",
      "additional_encoder.layers.4.linear1.bias torch.Size([2048])\n",
      "additional_encoder.layers.4.linear2.weight torch.Size([840, 2048])\n",
      "additional_encoder.layers.4.linear2.bias torch.Size([840])\n",
      "additional_encoder.layers.4.norm1.weight torch.Size([840])\n",
      "additional_encoder.layers.4.norm1.bias torch.Size([840])\n",
      "additional_encoder.layers.4.norm2.weight torch.Size([840])\n",
      "additional_encoder.layers.4.norm2.bias torch.Size([840])\n",
      "additional_encoder.layers.5.self_attn.in_proj_weight torch.Size([2520, 840])\n",
      "additional_encoder.layers.5.self_attn.in_proj_bias torch.Size([2520])\n",
      "additional_encoder.layers.5.self_attn.out_proj.weight torch.Size([840, 840])\n",
      "additional_encoder.layers.5.self_attn.out_proj.bias torch.Size([840])\n",
      "additional_encoder.layers.5.linear1.weight torch.Size([2048, 840])\n",
      "additional_encoder.layers.5.linear1.bias torch.Size([2048])\n",
      "additional_encoder.layers.5.linear2.weight torch.Size([840, 2048])\n",
      "additional_encoder.layers.5.linear2.bias torch.Size([840])\n",
      "additional_encoder.layers.5.norm1.weight torch.Size([840])\n",
      "additional_encoder.layers.5.norm1.bias torch.Size([840])\n",
      "additional_encoder.layers.5.norm2.weight torch.Size([840])\n",
      "additional_encoder.layers.5.norm2.bias torch.Size([840])\n",
      "fc1.weight torch.Size([1024, 840])\n",
      "fc1.bias torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = '/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/task1_task2_taskNRv2_finetune_BrainTranslator_skipstep1_b1_20_30_5e-05_5e-07_unique_sent.pt'  # Change to the path of your model\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "for layer_name, weight in checkpoint.items():\n",
    "    print(layer_name, weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint1 = torch.load('path_to_checkpoint1.pt', map_location=torch.device('cpu'))\n",
    "checkpoint2 = torch.load('path_to_checkpoint2.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "are_weights_same = True\n",
    "for (layer_name1, weight1), (layer_name2, weight2) in zip(checkpoint1.items(), checkpoint2.items()):\n",
    "    if not torch.equal(weight1, weight2):\n",
    "        are_weights_same = False\n",
    "        print(f\"Different weights in layer: {layer_name1}\")\n",
    "        \n",
    "if are_weights_same:\n",
    "    print(\"The weights in both checkpoints are the same.\")\n",
    "else:\n",
    "    print(\"The weights in the checkpoints are different.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
