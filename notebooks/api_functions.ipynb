{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # add parent directory to system path\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from model.model_loader import get_model\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "from model.brain_translator_model import BrainTranslator\n",
    "from handler.inference import infer\n",
    "from handler.generate_masks import generate_masks_from_embeddings\n",
    "from handler.handler import process_uploaded_file\n",
    "from transformers import BartForConditionalGeneration\n",
    "from model.model_loader import get_model\n",
    "import os\n",
    "import torch\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings not loading proper mask.\n",
    "\n",
    "\n",
    "## Next Question - Does the test embeddings and test mask etc, work with a loaded model? Did I do this correctly the first time?\n",
    "\n",
    "How to answer\n",
    "\n",
    "1. Load the direct stuff. If this is hard then,\n",
    "2. Re-Run the model decoding, in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_embeddings_from_file(filepath: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Load embeddings from a given JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - filepath (str): The path to the JSON file containing embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: A tensor containing the loaded embeddings.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r') as file:\n",
    "        embeddings_data = json.load(file)\n",
    "    return torch.tensor(embeddings_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_masks_from_embeddings(embeddings: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    Generate attention masks and their inverse for a given embeddings tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings (torch.Tensor): The embeddings tensor.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the attention mask and its inverse.\n",
    "    \"\"\"\n",
    "    # Assuming non-zero embeddings represent valid tokens and zeros represent padding\n",
    "    attn_mask = (embeddings.sum(dim=-1) != 0).float()\n",
    "    attn_mask_invert = 1.0 - attn_mask\n",
    "    return attn_mask, attn_mask_invert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "running_loss = 0.0\n",
    "pretrained_bart = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "\n",
    "# Iterate over data.\n",
    "sample_count = 0\n",
    "\n",
    "# Create a placeholder token\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "placeholder_token = tokenizer(\"<s>\", return_tensors=\"pt\")\n",
    "target_tokens_list = []\n",
    "target_string_list = []\n",
    "pred_tokens_list = []\n",
    "pred_string_list = []\n",
    "max_file_number=50\n",
    "\n",
    "model = get_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has weights loaded.\n",
      "Model is in training mode.\n"
     ]
    }
   ],
   "source": [
    "if bool(model.state_dict()):\n",
    "    print(\"Model has weights loaded.\")\n",
    "else:\n",
    "    print(\"Model does not have weights loaded.\")\n",
    "\n",
    "if not model.training:\n",
    "    print(\"Model is in evaluation mode.\")\n",
    "else:\n",
    "    print(\"Model is in training mode.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "api_functions.ipynb  make_masks.ipynb  notebook.ipynb  working_v1.ipynb\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../datasets/saved_data/input_mask_invert_1.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/media/my_nvme_drive/thoughtx/notebooks/api_functions.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/my_nvme_drive/thoughtx/notebooks/api_functions.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m../datasets/saved_data/input_embeddings_1.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/my_nvme_drive/thoughtx/notebooks/api_functions.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     input_embeddings_data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(file)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/my_nvme_drive/thoughtx/notebooks/api_functions.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39m../datasets/saved_data/input_mask_invert_1.json\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/my_nvme_drive/thoughtx/notebooks/api_functions.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     input_mask_invert_data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(file)\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/my_nvme_drive/thoughtx/notebooks/api_functions.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m../datasets/saved_data/input_masks_1.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../datasets/saved_data/input_mask_invert_1.json'"
     ]
    }
   ],
   "source": [
    "# Load the new JSON files and inspect their structure\n",
    "!ls\n",
    "with open(\"../datasets/saved_data/input_embeddings_1.json\", 'r') as file:\n",
    "    input_embeddings_data = json.load(file)\n",
    "\n",
    "with open(\"../datasets/saved_data/input_mask_invert_1.json\", 'r') as file:\n",
    "    input_mask_invert_data = json.load(file)\n",
    "\n",
    "with open(\"../datasets/saved_data/input_masks_1.json\", 'r') as file:\n",
    "    input_masks_data = json.load(file)\n",
    "\n",
    "input_embeddings_data, input_mask_invert_data, input_masks_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../datasets/saved_data/input_embeddings_1.json exists!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'input_embeddings_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/media/my_nvme_drive/thoughtx/notebooks/api_functions.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/my_nvme_drive/thoughtx/notebooks/api_functions.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m attn_mask, attn_mask_invert \u001b[39m=\u001b[39m generate_masks_from_embeddings(\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/my_nvme_drive/thoughtx/notebooks/api_functions.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m input_embeddings_data\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/my_nvme_drive/thoughtx/notebooks/api_functions.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/my_nvme_drive/thoughtx/notebooks/api_functions.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/my_nvme_drive/thoughtx/notebooks/api_functions.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m input_embeddings_tensor \u001b[39m=\u001b[39m input_embeddings_tensor\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/my_nvme_drive/thoughtx/notebooks/api_functions.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m input_masks_tensor \u001b[39m=\u001b[39m input_masks_tensor\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/my_nvme_drive/thoughtx/notebooks/api_functions.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m input_mask_invert_tensor \u001b[39m=\u001b[39m input_mask_invert_tensor\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_embeddings_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(1, max_file_number + 1):\n",
    "    # Step 3: Dynamically build the file paths based on the loop index\n",
    "    file = f\"../datasets/saved_data/input_embeddings_{i}.json\"# Process the uploaded EEG data file\n",
    "\n",
    "    if os.path.exists(file):\n",
    "        print(f\"File {file} exists!\")\n",
    "    else:\n",
    "        print(f\"File {file} does NOT exist!\")\n",
    "\n",
    "\n",
    "    input_embeddings_data =load_embeddings_from_file(file)\n",
    "\n",
    "    # Generate the necessary masks\n",
    "    attn_mask, attn_mask_invert = generate_masks_from_embeddings(\n",
    "    input_embeddings_data\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "    input_embeddings_tensor = input_embeddings_tensor\n",
    "    input_masks_tensor = input_masks_tensor\n",
    "    input_mask_invert_tensor = input_mask_invert_tensor\n",
    "\n",
    "    # Acquire the model and generate text\n",
    "    # model = BrainTranslator(pretrained_bart)\n",
    "    \n",
    "    # Step 5: Process the data with the model\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            outputs = model(input_embeddings_tensor, input_masks_tensor, input_mask_invert_tensor, placeholder_token[\"input_ids\"])\n",
    "            # Extract the generated token IDs from the model's outputs\n",
    "            logits=outputs.logits\n",
    "            probs = logits[0].softmax(dim = 1)\n",
    "            values, predictions = probs.topk(1)\n",
    "            predictions = torch.squeeze(predictions)\n",
    "            predicted_string = tokenizer.decode(predictions).split('</s></s>')[0].replace('<s>','')\n",
    "            predictions = predictions.tolist()\n",
    "            truncated_prediction = []\n",
    "            for t in predictions:\n",
    "                if t != tokenizer.eos_token_id:\n",
    "                    truncated_prediction.append(t)\n",
    "                else:\n",
    "                    break\n",
    "            pred_tokens = tokenizer.convert_ids_to_tokens(truncated_prediction, skip_special_tokens = True)\n",
    "            # print('predicted tokens:',pred_tokens)\n",
    "            pred_tokens_list.append(pred_tokens)\n",
    "            pred_string_list.append(predicted_string)\n",
    "            print('predicted string:',predicted_string)\n",
    "            # results.append(generated_text)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during inference: {str(e)}\")\n",
    "            results.append(str(e))\n",
    "\n",
    "\n",
    "pred_string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
