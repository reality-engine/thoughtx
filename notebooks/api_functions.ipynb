{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # add parent directory to system path\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from model.model_loader import get_model\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "from model.brain_translator_model import BrainTranslator\n",
    "from handler.inference import infer\n",
    "from handler.generate_masks import generate_masks_from_embeddings\n",
    "from handler.handler import process_uploaded_file\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "./datasets/saved_data/input_embeddings_50.json\n"
     ]
    }
   ],
   "source": [
    "max_file_number=50\n",
    "for i in range(1, max_file_number + 1):\n",
    "    # Step 3: Dynamically build the file paths based on the loop index\n",
    "    file = f\"../datasets/saved_data/input_embeddings_{i}.json\"\n",
    "    print(input_embeddings_path)\n",
    "    # Step 4: Load the data from these files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_embeddings_from_file(filepath: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Load embeddings from a given JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - filepath (str): The path to the JSON file containing embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: A tensor containing the loaded embeddings.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r') as file:\n",
    "        embeddings_data = json.load(file)\n",
    "    return torch.tensor(embeddings_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_masks_from_embeddings(embeddings: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    Generate attention masks and their inverse for a given embeddings tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings (torch.Tensor): The embeddings tensor.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the attention mask and its inverse.\n",
    "    \"\"\"\n",
    "    # Assuming non-zero embeddings represent valid tokens and zeros represent padding\n",
    "    attn_mask = (embeddings.sum(dim=-1) != 0).float()\n",
    "    attn_mask_invert = 1.0 - attn_mask\n",
    "    return attn_mask, attn_mask_invert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted string: TheThepper\n",
      "predicted string: ent\n",
      "predicted string: ItItIt\n",
      "predicted string: \n",
      "predicted string: \n",
      "predicted string: \n",
      "predicted string: ItItIt\n",
      "predicted string: i\n",
      "predicted string: \n",
      "predicted string: ItItIt\n",
      "predicted string: TheThe.\n",
      "predicted string: ItItIt\n",
      "predicted string: ItIt it\n",
      "predicted string: PP Doctor\n",
      "predicted string: \n",
      "predicted string:  back\n",
      "predicted string: is\n",
      "predicted string: ItItIt\n",
      "predicted string: III\n",
      "predicted string:  Deal\n",
      "predicted string:  back\n",
      "predicted string: TheTheThe\n",
      "predicted string: al\n",
      "predicted string: .\n",
      "predicted string: ItItIt\n",
      "predicted string: is\n",
      "predicted string: \n",
      "predicted string: is\n",
      "predicted string: \n",
      "predicted string:  Beach\n",
      "predicted string: ott\n",
      "predicted string:  i\n",
      "predicted string:  men\n",
      "predicted string: ine\n",
      "predicted string: ItItIt\n",
      "predicted string:  Johnson\n",
      "predicted string: \n",
      "predicted string: ,\n",
      "predicted string: \n",
      "predicted string: \n",
      "predicted string:  model\n",
      "predicted string: \n",
      "predicted string: rep\n",
      "predicted string: III\n",
      "predicted string:  group\n",
      "predicted string:  you\n",
      "predicted string: ItIt copy\n",
      "predicted string:  of\n",
      "predicted string: III\n",
      "predicted string: TheTheThe\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "running_loss = 0.0\n",
    "pretrained_bart = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "\n",
    "# Iterate over data.\n",
    "sample_count = 0\n",
    "\n",
    "# Create a placeholder token\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "placeholder_token = tokenizer(\"<s>\", return_tensors=\"pt\")\n",
    "target_tokens_list = []\n",
    "target_string_list = []\n",
    "pred_tokens_list = []\n",
    "pred_string_list = []\n",
    "max_file_number=50\n",
    "for i in range(1, max_file_number + 1):\n",
    "    # Step 3: Dynamically build the file paths based on the loop index\n",
    "    file = f\"../datasets/saved_data/input_embeddings_{i}.json\"# Process the uploaded EEG data file\n",
    "    input_embeddings_data =load_embeddings_from_file(file)\n",
    "\n",
    "    # Generate the necessary masks\n",
    "    attn_mask, attn_mask_invert = generate_masks_from_embeddings(\n",
    "    input_embeddings_data\n",
    "    )\n",
    "\n",
    "    # Acquire the model and generate text\n",
    "    model = BrainTranslator(pretrained_bart)\n",
    "    # Step 5: Process the data with the model\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            outputs = model(input_embeddings_tensor, input_masks_tensor, input_mask_invert_tensor, placeholder_token[\"input_ids\"])\n",
    "            # Extract the generated token IDs from the model's outputs\n",
    "            logits=outputs.logits\n",
    "            probs = logits[0].softmax(dim = 1)\n",
    "            values, predictions = probs.topk(1)\n",
    "            predictions = torch.squeeze(predictions)\n",
    "            predicted_string = tokenizer.decode(predictions).split('</s></s>')[0].replace('<s>','')\n",
    "            predictions = predictions.tolist()\n",
    "            truncated_prediction = []\n",
    "            for t in predictions:\n",
    "                if t != tokenizer.eos_token_id:\n",
    "                    truncated_prediction.append(t)\n",
    "                else:\n",
    "                    break\n",
    "            pred_tokens = tokenizer.convert_ids_to_tokens(truncated_prediction, skip_special_tokens = True)\n",
    "            # print('predicted tokens:',pred_tokens)\n",
    "            pred_tokens_list.append(pred_tokens)\n",
    "            pred_string_list.append(predicted_string)\n",
    "            print('predicted string:',predicted_string)\n",
    "            # results.append(generated_text)\n",
    "        except Exception as e:\n",
    "            results.append(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
