{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # add parent directory to system path\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from model.model_loader import get_model\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "from model.brain_translator_model import BrainTranslator\n",
    "from handler.inference import infer\n",
    "from handler.generate_masks import generate_masks_from_embeddings\n",
    "from handler.handler import process_uploaded_file\n",
    "from transformers import BartForConditionalGeneration\n",
    "from model.model_loader import get_model\n",
    "import os\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_embeddings_from_file(filepath: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Load embeddings from a given JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - filepath (str): The path to the JSON file containing embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: A tensor containing the loaded embeddings.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r') as file:\n",
    "        embeddings_data = json.load(file)\n",
    "    return torch.tensor(embeddings_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_masks_from_embeddings(embeddings: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    Generate attention masks and their inverse for a given embeddings tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings (torch.Tensor): The embeddings tensor.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the attention mask and its inverse.\n",
    "    \"\"\"\n",
    "    # Assuming non-zero embeddings represent valid tokens and zeros represent padding\n",
    "    attn_mask = (embeddings.sum(dim=-1) != 0).float()\n",
    "    attn_mask_invert = 1.0 - attn_mask\n",
    "    return attn_mask, attn_mask_invert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "running_loss = 0.0\n",
    "pretrained_bart = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "\n",
    "# Iterate over data.\n",
    "sample_count = 0\n",
    "\n",
    "# Create a placeholder token\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "placeholder_token = tokenizer(\"<s>\", return_tensors=\"pt\")\n",
    "target_tokens_list = []\n",
    "target_string_list = []\n",
    "pred_tokens_list = []\n",
    "pred_string_list = []\n",
    "max_file_number=50\n",
    "\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has weights loaded.\n",
      "Model is in evaluation mode.\n"
     ]
    }
   ],
   "source": [
    "if bool(model.state_dict()):\n",
    "    print(\"Model has weights loaded.\")\n",
    "else:\n",
    "    print(\"Model does not have weights loaded.\")\n",
    "\n",
    "if not model.training:\n",
    "    print(\"Model is in evaluation mode.\")\n",
    "else:\n",
    "    print(\"Model is in training mode.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../datasets/saved_data/input_embeddings_1.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_2.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_3.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_4.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_5.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_6.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_7.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_8.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_9.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_10.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_11.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_12.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_13.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_14.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_15.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_16.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_17.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_18.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_19.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_20.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_21.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_22.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_23.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_24.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_25.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_26.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_27.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_28.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_29.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_30.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_31.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_32.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_33.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_34.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_35.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_36.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_37.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_38.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_39.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_40.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_41.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_42.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_43.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_44.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_45.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_46.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_47.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_48.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_49.json exists!\n",
      "predicted string: \n",
      "File ../datasets/saved_data/input_embeddings_50.json exists!\n",
      "predicted string: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1, max_file_number + 1):\n",
    "    # Step 3: Dynamically build the file paths based on the loop index\n",
    "    file = f\"../datasets/saved_data/input_embeddings_{i}.json\"# Process the uploaded EEG data file\n",
    "\n",
    "    if os.path.exists(file):\n",
    "        print(f\"File {file} exists!\")\n",
    "    else:\n",
    "        print(f\"File {file} does NOT exist!\")\n",
    "\n",
    "\n",
    "    input_embeddings_data =load_embeddings_from_file(file)\n",
    "\n",
    "    # Generate the necessary masks\n",
    "    attn_mask, attn_mask_invert = generate_masks_from_embeddings(\n",
    "    input_embeddings_data\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "    input_embeddings_tensor = input_embeddings_tensor\n",
    "    input_masks_tensor = input_masks_tensor\n",
    "    input_mask_invert_tensor = input_mask_invert_tensor\n",
    "\n",
    "    # Acquire the model and generate text\n",
    "    # model = BrainTranslator(pretrained_bart)\n",
    "    \n",
    "    # Step 5: Process the data with the model\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            outputs = model(input_embeddings_tensor, input_masks_tensor, input_mask_invert_tensor, placeholder_token[\"input_ids\"])\n",
    "            # Extract the generated token IDs from the model's outputs\n",
    "            logits=outputs.logits\n",
    "            probs = logits[0].softmax(dim = 1)\n",
    "            values, predictions = probs.topk(1)\n",
    "            predictions = torch.squeeze(predictions)\n",
    "            predicted_string = tokenizer.decode(predictions).split('</s></s>')[0].replace('<s>','')\n",
    "            predictions = predictions.tolist()\n",
    "            truncated_prediction = []\n",
    "            for t in predictions:\n",
    "                if t != tokenizer.eos_token_id:\n",
    "                    truncated_prediction.append(t)\n",
    "                else:\n",
    "                    break\n",
    "            pred_tokens = tokenizer.convert_ids_to_tokens(truncated_prediction, skip_special_tokens = True)\n",
    "            # print('predicted tokens:',pred_tokens)\n",
    "            pred_tokens_list.append(pred_tokens)\n",
    "            pred_string_list.append(predicted_string)\n",
    "            print('predicted string:',predicted_string)\n",
    "            # results.append(generated_text)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during inference: {str(e)}\")\n",
    "            results.append(str(e))\n",
    "\n",
    "\n",
    "pred_string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = \"/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/datasets/datasets_eeg_text/zuco/task1-SR/pickle/task1-SR-dataset.pickle\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the pickled data into a DataFrame\n",
    "df = pd.read_pickle(pickle_file)\n",
    "\n",
    "# Get the first 100 rows\n",
    "# df = df.head(100)\n",
    "\n",
    "\n",
    "# # Print the first 100 rows\n",
    "# print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. data properties\n",
    "    1. Open the pickle, see the initial properties, consider re-running how the eval_decoding script is done.\n",
    "2. the conversion\n",
    "    1. Is the conversion to device important?\n",
    "3. in the model itself.\n",
    "    Is our model trash?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
