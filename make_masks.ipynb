{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in /opt/homebrew/lib/python3.11/site-packages (0.103.2)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: torch in /opt/homebrew/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: transformers in /opt/homebrew/lib/python3.11/site-packages (4.34.0)\n",
      "Requirement already satisfied: uvicorn in /opt/homebrew/lib/python3.11/site-packages (0.23.2)\n",
      "Requirement already satisfied: google-cloud-storage in /opt/homebrew/lib/python3.11/site-packages (2.11.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.11/site-packages (1.3.1)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /opt/homebrew/lib/python3.11/site-packages (from fastapi) (3.7.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /opt/homebrew/lib/python3.11/site-packages (from fastapi) (1.10.11)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/homebrew/lib/python3.11/site-packages (from fastapi) (0.27.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/homebrew/lib/python3.11/site-packages (from fastapi) (4.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michaelholborn/Library/Python/3.11/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (1.25.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/michaelholborn/Library/Python/3.11/lib/python/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/homebrew/lib/python3.11/site-packages (from uvicorn) (8.1.4)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/homebrew/lib/python3.11/site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/homebrew/lib/python3.11/site-packages (from google-cloud-storage) (2.23.3)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/homebrew/lib/python3.11/site-packages (from google-cloud-storage) (2.12.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/homebrew/lib/python3.11/site-packages (from google-cloud-storage) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/homebrew/lib/python3.11/site-packages (from google-cloud-storage) (2.6.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/lib/python3.11/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/lib/python3.11/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/homebrew/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.60.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /opt/homebrew/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (4.24.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/homebrew/lib/python3.11/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/homebrew/lib/python3.11/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.9)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from google-resumable-media>=2.6.0->google-cloud-storage) (1.5.0)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/michaelholborn/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/homebrew/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install fastapi pandas torch transformers uvicorn google-cloud-storage scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fastapi import FastAPI, UploadFile, HTTPException\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import io\n",
    "from transformers import BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "_MODEL = None\n",
    "\n",
    "def get_model():\n",
    "    global _MODEL\n",
    "\n",
    "    if _MODEL is None:\n",
    "        pretrained_bart = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "        checkpoint_path = '/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/task1_task2_taskNRv2_finetune_BrainTranslator_skipstep1_b1_20_30_5e-05_5e-07_unique_sent.pt'  # Change to the path of your model\n",
    "        _MODEL = BrainTranslator(pretrained_bart)\n",
    "        model_weights = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "        _MODEL.load_state_dict(model_weights)\n",
    "        _MODEL.eval()\n",
    "\n",
    "    return _MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the BrainTranslator model\n",
    "class BrainTranslator(nn.Module):\n",
    "    def __init__(self, pretrained_layers, in_feature=840, decoder_embedding_size=1024, additional_encoder_nhead=8, additional_encoder_dim_feedforward=2048):\n",
    "        super(BrainTranslator, self).__init__()\n",
    "        \n",
    "        self.pretrained = pretrained_layers\n",
    "        self.additional_encoder_layer = nn.TransformerEncoderLayer(d_model=in_feature, nhead=additional_encoder_nhead, dim_feedforward=additional_encoder_dim_feedforward, batch_first=True)\n",
    "        self.additional_encoder = nn.TransformerEncoder(self.additional_encoder_layer, num_layers=6)\n",
    "        self.fc1 = nn.Linear(in_feature, decoder_embedding_size)\n",
    "\n",
    "    def forward(self, input_embeddings_batch, input_masks_batch, input_mask_invert, decoder_input_ids):\n",
    "        encoded_embedding = self.additional_encoder(input_embeddings_batch, src_key_padding_mask=input_mask_invert)\n",
    "        encoded_embedding = F.relu(self.fc1(encoded_embedding))\n",
    "        out = self.pretrained(inputs_embeds=encoded_embedding, attention_mask=input_masks_batch, decoder_input_ids=decoder_input_ids, return_dict=True)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mask our own masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def load_embeddings_from_file(filepath: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Load embeddings from a given JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - filepath (str): The path to the JSON file containing embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: A tensor containing the loaded embeddings.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r') as file:\n",
    "        embeddings_data = json.load(file)\n",
    "    return torch.tensor(embeddings_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings_data=load_embeddings_from_file(\"./datasets/input_old_model/us-central1_eeg-test_dodadqada_dodadqada_saved_data_input_embeddings_12.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_masks_from_embeddings(embeddings: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    Generate attention masks and their inverse for a given embeddings tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings (torch.Tensor): The embeddings tensor.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the attention mask and its inverse.\n",
    "    \"\"\"\n",
    "    # Assuming non-zero embeddings represent valid tokens and zeros represent padding\n",
    "    attn_mask = (embeddings.sum(dim=-1) != 0).float()\n",
    "    attn_mask_invert = 1.0 - attn_mask\n",
    "    return attn_mask, attn_mask_invert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask,attn_mask_invert=generate_masks_from_embeddings(input_embeddings_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8m/y9dvzjx95lg__pqq1cypt21h0000gn/T/ipykernel_75901/2687417766.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_embeddings_tensor = torch.tensor(input_embeddings_data)\n",
      "/var/folders/8m/y9dvzjx95lg__pqq1cypt21h0000gn/T/ipykernel_75901/2687417766.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_masks_tensor = torch.tensor(attn_mask)\n",
      "/var/folders/8m/y9dvzjx95lg__pqq1cypt21h0000gn/T/ipykernel_75901/2687417766.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_mask_invert_tensor = torch.tensor(attn_mask_invert)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 56, 840]), torch.Size([1, 56]), torch.Size([1, 56]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# Convert loaded data to PyTorch tensors\n",
    "input_embeddings_tensor = torch.tensor(input_embeddings_data)\n",
    "input_masks_tensor = torch.tensor(attn_mask)\n",
    "input_mask_invert_tensor = torch.tensor(attn_mask_invert)\n",
    "\n",
    "input_embeddings_tensor.shape, input_masks_tensor.shape, input_mask_invert_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize model\n",
    "pretrained_bart = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "model = BrainTranslator(pretrained_bart)\n",
    "\n",
    "# Create a placeholder token\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "placeholder_token = tokenizer(\"<s>\", return_tensors=\"pt\")\n",
    "\n",
    "# Attempt a forward pass\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        outputs = model(input_embeddings_tensor, input_masks_tensor, input_mask_invert_tensor, placeholder_token[\"input_ids\"])\n",
    "        result = \"Forward pass successful!\"\n",
    "    except Exception as e:\n",
    "        result = str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result\n",
    "outputs\n",
    "# Extract the generated token IDs from the model's outputs\n",
    "generated_ids = outputs.logits.argmax(dim=-1)\n",
    "generated_ids\n",
    "\n",
    "# Decode the token IDs to text\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fastapi import FastAPI, UploadFile, HTTPException\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import io\n",
    "from transformers import BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "_MODEL = None\n",
    "\n",
    "def get_model():\n",
    "    global _MODEL\n",
    "\n",
    "    if _MODEL is None:\n",
    "        pretrained_bart = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "        checkpoint_path = '/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/task1_task2_taskNRv2_finetune_BrainTranslator_skipstep1_b1_20_30_5e-05_5e-07_unique_sent.pt'  # Change to the path of your model\n",
    "        _MODEL = BrainTranslator(pretrained_bart)\n",
    "        model_weights = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "        _MODEL.load_state_dict(model_weights)\n",
    "        _MODEL.eval()\n",
    "\n",
    "    return _MODEL\n",
    "\n",
    "\n",
    "def load_embeddings_from_file(filepath: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Load embeddings from a given JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - filepath (str): The path to the JSON file containing embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: A tensor containing the loaded embeddings.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r') as file:\n",
    "        embeddings_data = json.load(file)\n",
    "    return torch.tensor(embeddings_data)\n",
    "\n",
    "def generate_masks_from_embeddings(embeddings: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    Generate attention masks and their inverse for a given embeddings tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings (torch.Tensor): The embeddings tensor.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the attention mask and its inverse.\n",
    "    \"\"\"\n",
    "    # Assuming non-zero embeddings represent valid tokens and zeros represent padding\n",
    "    attn_mask = (embeddings.sum(dim=-1) != 0).float()\n",
    "    attn_mask_invert = 1.0 - attn_mask\n",
    "    return attn_mask, attn_mask_invert\n",
    "\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\"\"\" main architecture for open vocabulary EEG-To-Text decoding\"\"\"\n",
    "\n",
    "# Define the BrainTranslator model\n",
    "class BrainTranslator(nn.Module):\n",
    "    def __init__(self, pretrained_layers, in_feature=840, decoder_embedding_size=1024, additional_encoder_nhead=8, additional_encoder_dim_feedforward=2048):\n",
    "        super(BrainTranslator, self).__init__()\n",
    "        \n",
    "        self.pretrained = pretrained_layers\n",
    "        self.additional_encoder_layer = nn.TransformerEncoderLayer(d_model=in_feature, nhead=additional_encoder_nhead, dim_feedforward=additional_encoder_dim_feedforward, batch_first=True)\n",
    "        self.additional_encoder = nn.TransformerEncoder(self.additional_encoder_layer, num_layers=6)\n",
    "        self.fc1 = nn.Linear(in_feature, decoder_embedding_size)\n",
    "\n",
    "    def forward(self, input_embeddings_batch, input_masks_batch, input_mask_invert, decoder_input_ids):\n",
    "        encoded_embedding = self.additional_encoder(input_embeddings_batch, src_key_padding_mask=input_mask_invert)\n",
    "        encoded_embedding = F.relu(self.fc1(encoded_embedding))\n",
    "        out = self.pretrained(inputs_embeds=encoded_embedding, attention_mask=input_masks_batch, decoder_input_ids=decoder_input_ids, return_dict=True)\n",
    "        return out\n",
    "\n",
    "\n",
    "# class BrainTranslator(nn.Module):\n",
    "#     def __init__(self, pretrained_layers, in_feature = 840, decoder_embedding_size = 1024, additional_encoder_nhead=8, additional_encoder_dim_feedforward = 2048):\n",
    "#         super(BrainTranslator, self).__init__()\n",
    "        \n",
    "#         self.pretrained = pretrained_layers\n",
    "#         # additional transformer encoder, following BART paper about \n",
    "#         self.additional_encoder_layer = nn.TransformerEncoderLayer(d_model=in_feature, nhead=additional_encoder_nhead,  dim_feedforward = additional_encoder_dim_feedforward, batch_first=True)\n",
    "#         self.additional_encoder = nn.TransformerEncoder(self.additional_encoder_layer, num_layers=6)\n",
    "        \n",
    "#         # print('[INFO]adding positional embedding')\n",
    "#         # self.positional_embedding = PositionalEncoding(in_feature)\n",
    "\n",
    "#         self.fc1 = nn.Linear(in_feature, decoder_embedding_size)\n",
    "\n",
    "#     def forward(self, input_embeddings_batch, input_masks_batch, input_masks_invert, target_ids_batch_converted):\n",
    "#         \"\"\"input_embeddings_batch: batch_size*Seq_len*840\"\"\"\n",
    "#         \"\"\"input_mask: 1 is not masked, 0 is masked\"\"\"\n",
    "#         \"\"\"input_masks_invert: 1 is masked, 0 is not masked\"\"\"\n",
    "        \n",
    "#         # input_embeddings_batch = self.positional_embedding(input_embeddings_batch) \n",
    "\n",
    "#         # use src_key_padding_masks\n",
    "#         encoded_embedding = self.additional_encoder(input_embeddings_batch, src_key_padding_mask = input_masks_invert) \n",
    "        \n",
    "#         # encoded_embedding = self.additional_encoder(input_embeddings_batch) \n",
    "#         encoded_embedding = F.relu(self.fc1(encoded_embedding))\n",
    "#         out = self.pretrained(inputs_embeds = encoded_embedding, attention_mask = input_masks_batch, return_dict = True, labels = target_ids_batch_converted)                    \n",
    "        \n",
    "#         return out\n",
    "\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "pretrained_bart = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "model = BrainTranslator(pretrained_bart)\n",
    "\n",
    "\n",
    "# model = BrainTranslator(pretrained_bart, in_feature = 105*len([\n",
    "#         \"_t1\",\n",
    "#         \"_t2\",\n",
    "#         \"_a1\",\n",
    "#         \"_a2\",\n",
    "#         \"_b1\",\n",
    "#         \"_b2\",\n",
    "#         \"_g1\",\n",
    "#         \"_g2\"\n",
    "#     ]), decoder_embedding_size = 1024, additional_encoder_nhead=8, additional_encoder_dim_feedforward = 2048)\n",
    "\n",
    "\n",
    "# Create a placeholder token\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "placeholder_token = tokenizer(\"<s>\", return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/saved_data/input_embeddings_1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8m/y9dvzjx95lg__pqq1cypt21h0000gn/T/ipykernel_75901/2822111578.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_embeddings_tensor = torch.tensor(input_embeddings_data)\n",
      "/var/folders/8m/y9dvzjx95lg__pqq1cypt21h0000gn/T/ipykernel_75901/2822111578.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_masks_tensor = torch.tensor(input_masks_data)\n",
      "/var/folders/8m/y9dvzjx95lg__pqq1cypt21h0000gn/T/ipykernel_75901/2822111578.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_mask_invert_tensor = torch.tensor(input_mask_invert_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted string: It\n",
      "./datasets/saved_data/input_embeddings_2.json\n",
      "predicted string:  State\n",
      "./datasets/saved_data/input_embeddings_3.json\n",
      "predicted string:  you\n",
      "./datasets/saved_data/input_embeddings_4.json\n",
      "predicted string:  Rep\n",
      "./datasets/saved_data/input_embeddings_5.json\n",
      "predicted string: is\n",
      "./datasets/saved_data/input_embeddings_6.json\n",
      "predicted string: is\n",
      "./datasets/saved_data/input_embeddings_7.json\n",
      "predicted string: es\n",
      "./datasets/saved_data/input_embeddings_8.json\n",
      "predicted string:  Damage\n",
      "./datasets/saved_data/input_embeddings_9.json\n",
      "predicted string: irl\n",
      "./datasets/saved_data/input_embeddings_10.json\n",
      "predicted string:  Price\n",
      "./datasets/saved_data/input_embeddings_11.json\n",
      "predicted string:  Pier\n",
      "./datasets/saved_data/input_embeddings_12.json\n",
      "predicted string: is\n",
      "./datasets/saved_data/input_embeddings_13.json\n",
      "predicted string:  Like\n",
      "./datasets/saved_data/input_embeddings_14.json\n",
      "predicted string:  State\n",
      "./datasets/saved_data/input_embeddings_15.json\n",
      "predicted string: VerVer Bird\n",
      "./datasets/saved_data/input_embeddings_16.json\n",
      "predicted string:  Pier\n",
      "./datasets/saved_data/input_embeddings_17.json\n",
      "predicted string: en\n",
      "./datasets/saved_data/input_embeddings_18.json\n",
      "predicted string:  held\n",
      "./datasets/saved_data/input_embeddings_19.json\n",
      "predicted string:  Pier\n",
      "./datasets/saved_data/input_embeddings_20.json\n",
      "predicted string: is\n",
      "./datasets/saved_data/input_embeddings_21.json\n",
      "predicted string:  else\n",
      "./datasets/saved_data/input_embeddings_22.json\n",
      "predicted string: This\n",
      "./datasets/saved_data/input_embeddings_23.json\n",
      "predicted string:  Pepper\n",
      "./datasets/saved_data/input_embeddings_24.json\n",
      "predicted string: ad\n",
      "./datasets/saved_data/input_embeddings_25.json\n",
      "predicted string: ad\n",
      "./datasets/saved_data/input_embeddings_26.json\n",
      "predicted string:  \n",
      "./datasets/saved_data/input_embeddings_27.json\n",
      "predicted string: </s>\n",
      "./datasets/saved_data/input_embeddings_28.json\n",
      "predicted string:  left\n",
      "./datasets/saved_data/input_embeddings_29.json\n",
      "predicted string:  Plane\n",
      "./datasets/saved_data/input_embeddings_30.json\n",
      "predicted string: is\n",
      "./datasets/saved_data/input_embeddings_31.json\n",
      "predicted string: is\n",
      "./datasets/saved_data/input_embeddings_32.json\n",
      "predicted string: is\n",
      "./datasets/saved_data/input_embeddings_33.json\n",
      "predicted string: is\n",
      "./datasets/saved_data/input_embeddings_34.json\n",
      "predicted string:  must\n",
      "./datasets/saved_data/input_embeddings_35.json\n",
      "predicted string:  Never\n",
      "./datasets/saved_data/input_embeddings_36.json\n",
      "predicted string: I\n",
      "./datasets/saved_data/input_embeddings_37.json\n",
      "predicted string: pe\n",
      "./datasets/saved_data/input_embeddings_38.json\n",
      "predicted string:  I\n",
      "./datasets/saved_data/input_embeddings_39.json\n",
      "predicted string: is\n",
      "./datasets/saved_data/input_embeddings_40.json\n",
      "predicted string: is\n",
      "./datasets/saved_data/input_embeddings_41.json\n",
      "predicted string:  do\n",
      "./datasets/saved_data/input_embeddings_42.json\n",
      "predicted string:  re\n",
      "./datasets/saved_data/input_embeddings_43.json\n",
      "predicted string: ItIt -\n",
      "./datasets/saved_data/input_embeddings_44.json\n",
      "predicted string: II I\n",
      "./datasets/saved_data/input_embeddings_45.json\n",
      "predicted string: ItIt people\n",
      "./datasets/saved_data/input_embeddings_46.json\n",
      "predicted string:  Matt\n",
      "./datasets/saved_data/input_embeddings_47.json\n",
      "predicted string: .\n",
      "./datasets/saved_data/input_embeddings_48.json\n",
      "predicted string: \"\"is\n",
      "./datasets/saved_data/input_embeddings_49.json\n",
      "predicted string: II I\n",
      "./datasets/saved_data/input_embeddings_50.json\n",
      "predicted string: ThisThis him\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['It', ' State', ' you', ' Rep', 'is', 'is', 'es', ' Damage', 'irl', ' Price']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "\n",
    "# Iterate over data.\n",
    "sample_count = 0\n",
    "\n",
    "target_tokens_list = []\n",
    "target_string_list = []\n",
    "pred_tokens_list = []\n",
    "pred_string_list = []\n",
    "# Step 2: Create a loop to process each file\n",
    "results = []\n",
    "max_file_number=50\n",
    "for i in range(1, max_file_number + 1):\n",
    "    # Step 3: Dynamically build the file paths based on the loop index\n",
    "    input_embeddings_path = f\"./datasets/saved_data/input_embeddings_{i}.json\"\n",
    "    print(input_embeddings_path)\n",
    "    # Step 4: Load the data from these files\n",
    "    try:\n",
    "       input_embeddings_data=load_embeddings_from_file(input_embeddings_path)\n",
    "       input_masks_data,input_mask_invert_data=generate_masks_from_embeddings(input_embeddings_data)\n",
    "    except FileNotFoundError:\n",
    "        # If one of the files isn't found, we'll append an error message and continue to the next iteration\n",
    "        results.append(f\"Files for index {i} not found.\")\n",
    "        continue\n",
    "\n",
    "    # Convert loaded data to PyTorch tensors\n",
    "    input_embeddings_tensor = torch.tensor(input_embeddings_data)\n",
    "    input_masks_tensor = torch.tensor(input_masks_data)\n",
    "    input_mask_invert_tensor = torch.tensor(input_mask_invert_data)\n",
    "    \n",
    "    # Step 5: Process the data with the model\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            outputs = model(input_embeddings_tensor, input_masks_tensor, input_mask_invert_tensor, placeholder_token[\"input_ids\"])\n",
    "            # Extract the generated token IDs from the model's outputs\n",
    "            logits=outputs.logits\n",
    "            probs = logits[0].softmax(dim = 1)\n",
    "            values, predictions = probs.topk(1)\n",
    "            predictions = torch.squeeze(predictions)\n",
    "            predicted_string = tokenizer.decode(predictions).split('</s></s>')[0].replace('<s>','')\n",
    "            predictions = predictions.tolist()\n",
    "            truncated_prediction = []\n",
    "            for t in predictions:\n",
    "                if t != tokenizer.eos_token_id:\n",
    "                    truncated_prediction.append(t)\n",
    "                else:\n",
    "                    break\n",
    "            pred_tokens = tokenizer.convert_ids_to_tokens(truncated_prediction, skip_special_tokens = True)\n",
    "            # print('predicted tokens:',pred_tokens)\n",
    "            pred_tokens_list.append(pred_tokens)\n",
    "            pred_string_list.append(predicted_string)\n",
    "            print('predicted string:',predicted_string)\n",
    "            # results.append(generated_text)\n",
    "        except Exception as e:\n",
    "            results.append(str(e))\n",
    "\n",
    "pred_string_list[:10]  # Display the first 10 results for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " ' State',\n",
       " ' you',\n",
       " ' Rep',\n",
       " 'is',\n",
       " 'is',\n",
       " 'es',\n",
       " ' Damage',\n",
       " 'irl',\n",
       " ' Price',\n",
       " ' Pier',\n",
       " 'is',\n",
       " ' Like',\n",
       " ' State',\n",
       " 'VerVer Bird',\n",
       " ' Pier',\n",
       " 'en',\n",
       " ' held',\n",
       " ' Pier',\n",
       " 'is',\n",
       " ' else',\n",
       " 'This',\n",
       " ' Pepper',\n",
       " 'ad',\n",
       " 'ad',\n",
       " ' ',\n",
       " '</s>',\n",
       " ' left',\n",
       " ' Plane',\n",
       " 'is',\n",
       " 'is',\n",
       " 'is',\n",
       " 'is',\n",
       " ' must',\n",
       " ' Never',\n",
       " 'I',\n",
       " 'pe',\n",
       " ' I',\n",
       " 'is',\n",
       " 'is',\n",
       " ' do',\n",
       " ' re',\n",
       " 'ItIt -',\n",
       " 'II I',\n",
       " 'ItIt people',\n",
       " ' Matt',\n",
       " '.',\n",
       " '\"\"is',\n",
       " 'II I',\n",
       " 'ThisThis him']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained.final_logits_bias torch.Size([1, 50265])\n",
      "pretrained.model.shared.weight torch.Size([50265, 1024])\n",
      "pretrained.model.encoder.embed_tokens.weight torch.Size([50265, 1024])\n",
      "pretrained.model.encoder.embed_positions.weight torch.Size([1026, 1024])\n",
      "pretrained.model.encoder.layers.0.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.0.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.0.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.0.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.0.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.0.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.0.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.0.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.0.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.0.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.0.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.encoder.layers.0.fc1.bias torch.Size([4096])\n",
      "pretrained.model.encoder.layers.0.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.encoder.layers.0.fc2.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.0.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.0.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.1.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.1.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.1.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.1.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.1.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.1.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.1.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.1.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.1.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.1.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.1.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.encoder.layers.1.fc1.bias torch.Size([4096])\n",
      "pretrained.model.encoder.layers.1.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.encoder.layers.1.fc2.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.1.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.1.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.2.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.2.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.2.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.2.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.2.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.2.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.2.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.2.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.2.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.2.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.2.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.encoder.layers.2.fc1.bias torch.Size([4096])\n",
      "pretrained.model.encoder.layers.2.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.encoder.layers.2.fc2.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.2.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.2.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.3.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.3.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.3.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.3.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.3.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.3.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.3.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.3.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.3.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.3.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.3.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.encoder.layers.3.fc1.bias torch.Size([4096])\n",
      "pretrained.model.encoder.layers.3.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.encoder.layers.3.fc2.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.3.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.3.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.4.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.4.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.4.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.4.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.4.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.4.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.4.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.4.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.4.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.4.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.4.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.encoder.layers.4.fc1.bias torch.Size([4096])\n",
      "pretrained.model.encoder.layers.4.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.encoder.layers.4.fc2.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.4.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.4.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.5.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.5.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.5.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.5.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.5.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.5.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.5.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.5.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.5.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.5.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.5.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.encoder.layers.5.fc1.bias torch.Size([4096])\n",
      "pretrained.model.encoder.layers.5.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.encoder.layers.5.fc2.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.5.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.5.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.6.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.6.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.6.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.6.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.6.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.6.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.6.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.6.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.6.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.6.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.6.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.encoder.layers.6.fc1.bias torch.Size([4096])\n",
      "pretrained.model.encoder.layers.6.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.encoder.layers.6.fc2.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.6.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.6.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.7.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.7.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.7.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.7.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.7.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.7.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.7.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.7.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.7.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.7.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.7.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.encoder.layers.7.fc1.bias torch.Size([4096])\n",
      "pretrained.model.encoder.layers.7.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.encoder.layers.7.fc2.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.7.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.7.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.8.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.8.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.8.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.8.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.8.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.8.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.8.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.8.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.8.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.8.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.8.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.encoder.layers.8.fc1.bias torch.Size([4096])\n",
      "pretrained.model.encoder.layers.8.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.encoder.layers.8.fc2.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.8.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.8.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.9.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.9.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.9.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.9.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.9.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.9.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.9.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.9.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.9.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.9.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.9.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.encoder.layers.9.fc1.bias torch.Size([4096])\n",
      "pretrained.model.encoder.layers.9.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.encoder.layers.9.fc2.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.9.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.9.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.10.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.10.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.10.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.10.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.10.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.10.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.10.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.10.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.10.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.10.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.10.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.encoder.layers.10.fc1.bias torch.Size([4096])\n",
      "pretrained.model.encoder.layers.10.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.encoder.layers.10.fc2.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.10.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.10.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.11.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.11.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.11.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.11.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.11.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.11.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.11.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.encoder.layers.11.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.11.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.11.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.11.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.encoder.layers.11.fc1.bias torch.Size([4096])\n",
      "pretrained.model.encoder.layers.11.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.encoder.layers.11.fc2.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layers.11.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layers.11.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.encoder.layernorm_embedding.weight torch.Size([1024])\n",
      "pretrained.model.encoder.layernorm_embedding.bias torch.Size([1024])\n",
      "pretrained.model.decoder.embed_tokens.weight torch.Size([50265, 1024])\n",
      "pretrained.model.decoder.embed_positions.weight torch.Size([1026, 1024])\n",
      "pretrained.model.decoder.layers.0.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.0.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.0.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.0.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.0.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.0.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.0.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.0.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.0.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.decoder.layers.0.fc1.bias torch.Size([4096])\n",
      "pretrained.model.decoder.layers.0.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.decoder.layers.0.fc2.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.0.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.1.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.1.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.1.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.1.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.1.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.1.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.1.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.1.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.decoder.layers.1.fc1.bias torch.Size([4096])\n",
      "pretrained.model.decoder.layers.1.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.decoder.layers.1.fc2.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.1.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.2.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.2.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.2.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.2.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.2.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.2.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.2.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.2.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.decoder.layers.2.fc1.bias torch.Size([4096])\n",
      "pretrained.model.decoder.layers.2.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.decoder.layers.2.fc2.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.2.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.3.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.3.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.3.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.3.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.3.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.3.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.3.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.3.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.decoder.layers.3.fc1.bias torch.Size([4096])\n",
      "pretrained.model.decoder.layers.3.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.decoder.layers.3.fc2.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.3.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.4.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.4.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.4.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.4.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.4.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.4.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.4.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.4.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.decoder.layers.4.fc1.bias torch.Size([4096])\n",
      "pretrained.model.decoder.layers.4.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.decoder.layers.4.fc2.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.4.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.5.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.5.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.5.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.5.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.5.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.5.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.5.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.5.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.decoder.layers.5.fc1.bias torch.Size([4096])\n",
      "pretrained.model.decoder.layers.5.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.decoder.layers.5.fc2.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.5.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.6.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.6.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.6.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.6.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.6.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.6.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.6.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.6.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.decoder.layers.6.fc1.bias torch.Size([4096])\n",
      "pretrained.model.decoder.layers.6.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.decoder.layers.6.fc2.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.6.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.7.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.7.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.7.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.7.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.7.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.7.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.7.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.7.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.decoder.layers.7.fc1.bias torch.Size([4096])\n",
      "pretrained.model.decoder.layers.7.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.decoder.layers.7.fc2.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.7.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.8.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.8.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.8.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.8.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.8.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.8.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.8.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.8.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.decoder.layers.8.fc1.bias torch.Size([4096])\n",
      "pretrained.model.decoder.layers.8.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.decoder.layers.8.fc2.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.8.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.9.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.9.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.9.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.9.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.9.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.9.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.9.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.9.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.decoder.layers.9.fc1.bias torch.Size([4096])\n",
      "pretrained.model.decoder.layers.9.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.decoder.layers.9.fc2.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.9.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.10.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.10.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.10.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.10.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.10.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.10.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.10.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.10.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.decoder.layers.10.fc1.bias torch.Size([4096])\n",
      "pretrained.model.decoder.layers.10.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.decoder.layers.10.fc2.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.10.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.self_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.11.self_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.self_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.11.self_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.self_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.11.self_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.self_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.11.self_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.self_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.self_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.encoder_attn.k_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.11.encoder_attn.k_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.encoder_attn.v_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.11.encoder_attn.v_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.encoder_attn.q_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.11.encoder_attn.q_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.encoder_attn.out_proj.weight torch.Size([1024, 1024])\n",
      "pretrained.model.decoder.layers.11.encoder_attn.out_proj.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.encoder_attn_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.encoder_attn_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.fc1.weight torch.Size([4096, 1024])\n",
      "pretrained.model.decoder.layers.11.fc1.bias torch.Size([4096])\n",
      "pretrained.model.decoder.layers.11.fc2.weight torch.Size([1024, 4096])\n",
      "pretrained.model.decoder.layers.11.fc2.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.final_layer_norm.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layers.11.final_layer_norm.bias torch.Size([1024])\n",
      "pretrained.model.decoder.layernorm_embedding.weight torch.Size([1024])\n",
      "pretrained.model.decoder.layernorm_embedding.bias torch.Size([1024])\n",
      "pretrained.lm_head.weight torch.Size([50265, 1024])\n",
      "additional_encoder_layer.self_attn.in_proj_weight torch.Size([2520, 840])\n",
      "additional_encoder_layer.self_attn.in_proj_bias torch.Size([2520])\n",
      "additional_encoder_layer.self_attn.out_proj.weight torch.Size([840, 840])\n",
      "additional_encoder_layer.self_attn.out_proj.bias torch.Size([840])\n",
      "additional_encoder_layer.linear1.weight torch.Size([2048, 840])\n",
      "additional_encoder_layer.linear1.bias torch.Size([2048])\n",
      "additional_encoder_layer.linear2.weight torch.Size([840, 2048])\n",
      "additional_encoder_layer.linear2.bias torch.Size([840])\n",
      "additional_encoder_layer.norm1.weight torch.Size([840])\n",
      "additional_encoder_layer.norm1.bias torch.Size([840])\n",
      "additional_encoder_layer.norm2.weight torch.Size([840])\n",
      "additional_encoder_layer.norm2.bias torch.Size([840])\n",
      "additional_encoder.layers.0.self_attn.in_proj_weight torch.Size([2520, 840])\n",
      "additional_encoder.layers.0.self_attn.in_proj_bias torch.Size([2520])\n",
      "additional_encoder.layers.0.self_attn.out_proj.weight torch.Size([840, 840])\n",
      "additional_encoder.layers.0.self_attn.out_proj.bias torch.Size([840])\n",
      "additional_encoder.layers.0.linear1.weight torch.Size([2048, 840])\n",
      "additional_encoder.layers.0.linear1.bias torch.Size([2048])\n",
      "additional_encoder.layers.0.linear2.weight torch.Size([840, 2048])\n",
      "additional_encoder.layers.0.linear2.bias torch.Size([840])\n",
      "additional_encoder.layers.0.norm1.weight torch.Size([840])\n",
      "additional_encoder.layers.0.norm1.bias torch.Size([840])\n",
      "additional_encoder.layers.0.norm2.weight torch.Size([840])\n",
      "additional_encoder.layers.0.norm2.bias torch.Size([840])\n",
      "additional_encoder.layers.1.self_attn.in_proj_weight torch.Size([2520, 840])\n",
      "additional_encoder.layers.1.self_attn.in_proj_bias torch.Size([2520])\n",
      "additional_encoder.layers.1.self_attn.out_proj.weight torch.Size([840, 840])\n",
      "additional_encoder.layers.1.self_attn.out_proj.bias torch.Size([840])\n",
      "additional_encoder.layers.1.linear1.weight torch.Size([2048, 840])\n",
      "additional_encoder.layers.1.linear1.bias torch.Size([2048])\n",
      "additional_encoder.layers.1.linear2.weight torch.Size([840, 2048])\n",
      "additional_encoder.layers.1.linear2.bias torch.Size([840])\n",
      "additional_encoder.layers.1.norm1.weight torch.Size([840])\n",
      "additional_encoder.layers.1.norm1.bias torch.Size([840])\n",
      "additional_encoder.layers.1.norm2.weight torch.Size([840])\n",
      "additional_encoder.layers.1.norm2.bias torch.Size([840])\n",
      "additional_encoder.layers.2.self_attn.in_proj_weight torch.Size([2520, 840])\n",
      "additional_encoder.layers.2.self_attn.in_proj_bias torch.Size([2520])\n",
      "additional_encoder.layers.2.self_attn.out_proj.weight torch.Size([840, 840])\n",
      "additional_encoder.layers.2.self_attn.out_proj.bias torch.Size([840])\n",
      "additional_encoder.layers.2.linear1.weight torch.Size([2048, 840])\n",
      "additional_encoder.layers.2.linear1.bias torch.Size([2048])\n",
      "additional_encoder.layers.2.linear2.weight torch.Size([840, 2048])\n",
      "additional_encoder.layers.2.linear2.bias torch.Size([840])\n",
      "additional_encoder.layers.2.norm1.weight torch.Size([840])\n",
      "additional_encoder.layers.2.norm1.bias torch.Size([840])\n",
      "additional_encoder.layers.2.norm2.weight torch.Size([840])\n",
      "additional_encoder.layers.2.norm2.bias torch.Size([840])\n",
      "additional_encoder.layers.3.self_attn.in_proj_weight torch.Size([2520, 840])\n",
      "additional_encoder.layers.3.self_attn.in_proj_bias torch.Size([2520])\n",
      "additional_encoder.layers.3.self_attn.out_proj.weight torch.Size([840, 840])\n",
      "additional_encoder.layers.3.self_attn.out_proj.bias torch.Size([840])\n",
      "additional_encoder.layers.3.linear1.weight torch.Size([2048, 840])\n",
      "additional_encoder.layers.3.linear1.bias torch.Size([2048])\n",
      "additional_encoder.layers.3.linear2.weight torch.Size([840, 2048])\n",
      "additional_encoder.layers.3.linear2.bias torch.Size([840])\n",
      "additional_encoder.layers.3.norm1.weight torch.Size([840])\n",
      "additional_encoder.layers.3.norm1.bias torch.Size([840])\n",
      "additional_encoder.layers.3.norm2.weight torch.Size([840])\n",
      "additional_encoder.layers.3.norm2.bias torch.Size([840])\n",
      "additional_encoder.layers.4.self_attn.in_proj_weight torch.Size([2520, 840])\n",
      "additional_encoder.layers.4.self_attn.in_proj_bias torch.Size([2520])\n",
      "additional_encoder.layers.4.self_attn.out_proj.weight torch.Size([840, 840])\n",
      "additional_encoder.layers.4.self_attn.out_proj.bias torch.Size([840])\n",
      "additional_encoder.layers.4.linear1.weight torch.Size([2048, 840])\n",
      "additional_encoder.layers.4.linear1.bias torch.Size([2048])\n",
      "additional_encoder.layers.4.linear2.weight torch.Size([840, 2048])\n",
      "additional_encoder.layers.4.linear2.bias torch.Size([840])\n",
      "additional_encoder.layers.4.norm1.weight torch.Size([840])\n",
      "additional_encoder.layers.4.norm1.bias torch.Size([840])\n",
      "additional_encoder.layers.4.norm2.weight torch.Size([840])\n",
      "additional_encoder.layers.4.norm2.bias torch.Size([840])\n",
      "additional_encoder.layers.5.self_attn.in_proj_weight torch.Size([2520, 840])\n",
      "additional_encoder.layers.5.self_attn.in_proj_bias torch.Size([2520])\n",
      "additional_encoder.layers.5.self_attn.out_proj.weight torch.Size([840, 840])\n",
      "additional_encoder.layers.5.self_attn.out_proj.bias torch.Size([840])\n",
      "additional_encoder.layers.5.linear1.weight torch.Size([2048, 840])\n",
      "additional_encoder.layers.5.linear1.bias torch.Size([2048])\n",
      "additional_encoder.layers.5.linear2.weight torch.Size([840, 2048])\n",
      "additional_encoder.layers.5.linear2.bias torch.Size([840])\n",
      "additional_encoder.layers.5.norm1.weight torch.Size([840])\n",
      "additional_encoder.layers.5.norm1.bias torch.Size([840])\n",
      "additional_encoder.layers.5.norm2.weight torch.Size([840])\n",
      "additional_encoder.layers.5.norm2.bias torch.Size([840])\n",
      "fc1.weight torch.Size([1024, 840])\n",
      "fc1.bias torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = '/Users/michaelholborn/Documents/SoftwareLocal/monotropism/thoughtx/task1_task2_taskNRv2_finetune_BrainTranslator_skipstep1_b1_20_30_5e-05_5e-07_unique_sent.pt'  # Change to the path of your model\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "for layer_name, weight in checkpoint.items():\n",
    "    print(layer_name, weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint1 = torch.load('path_to_checkpoint1.pt', map_location=torch.device('cpu'))\n",
    "checkpoint2 = torch.load('path_to_checkpoint2.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "are_weights_same = True\n",
    "for (layer_name1, weight1), (layer_name2, weight2) in zip(checkpoint1.items(), checkpoint2.items()):\n",
    "    if not torch.equal(weight1, weight2):\n",
    "        are_weights_same = False\n",
    "        print(f\"Different weights in layer: {layer_name1}\")\n",
    "        \n",
    "if are_weights_same:\n",
    "    print(\"The weights in both checkpoints are the same.\")\n",
    "else:\n",
    "    print(\"The weights in the checkpoints are different.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
